{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337bf450-ac2d-45b0-ab94-eaa6a14b6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7823a8f-bedb-49e0-a0d7-d8bdb7026b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "fdf = pd.read_csv('fdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1bbdc1-5270-423f-8e82-5f5c60c1b788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_wt</th>\n",
       "      <th>age</th>\n",
       "      <th>age_re</th>\n",
       "      <th>b4_flow</th>\n",
       "      <th>b4_win</th>\n",
       "      <th>colour</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>course</th>\n",
       "      <th>dam's sire</th>\n",
       "      <th>date</th>\n",
       "      <th>declarhorse_wt</th>\n",
       "      <th>dist</th>\n",
       "      <th>draw</th>\n",
       "      <th>draw_counts</th>\n",
       "      <th>gear_list</th>\n",
       "      <th>going</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>horse_name</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>hv_rain</th>\n",
       "      <th>hv_temp</th>\n",
       "      <th>import type</th>\n",
       "      <th>jockey</th>\n",
       "      <th>last rating</th>\n",
       "      <th>n_raceindex</th>\n",
       "      <th>no. of 1-2-3-starts*</th>\n",
       "      <th>p_odds</th>\n",
       "      <th>p_odds_allo</th>\n",
       "      <th>p_trainer_counts</th>\n",
       "      <th>pace_sp</th>\n",
       "      <th>pace_sp_shifted</th>\n",
       "      <th>place</th>\n",
       "      <th>placing</th>\n",
       "      <th>placing_shifted</th>\n",
       "      <th>prize</th>\n",
       "      <th>q_odds</th>\n",
       "      <th>qp_odds</th>\n",
       "      <th>quinella</th>\n",
       "      <th>quinella place</th>\n",
       "      <th>race</th>\n",
       "      <th>raceclass</th>\n",
       "      <th>rc</th>\n",
       "      <th>rc_counts</th>\n",
       "      <th>result_counts</th>\n",
       "      <th>rtg.</th>\n",
       "      <th>rtg_race</th>\n",
       "      <th>same sire</th>\n",
       "      <th>same_sire_score</th>\n",
       "      <th>season stakes*</th>\n",
       "      <th>sex</th>\n",
       "      <th>sire</th>\n",
       "      <th>sire_score</th>\n",
       "      <th>st_rain</th>\n",
       "      <th>st_temp</th>\n",
       "      <th>start ofseason rating</th>\n",
       "      <th>time_per_distance</th>\n",
       "      <th>total stakes*</th>\n",
       "      <th>track</th>\n",
       "      <th>track_counts</th>\n",
       "      <th>train_gear</th>\n",
       "      <th>trainer</th>\n",
       "      <th>trot_sum</th>\n",
       "      <th>type_counts</th>\n",
       "      <th>venue_counts</th>\n",
       "      <th>w_odds</th>\n",
       "      <th>w_odds_allo</th>\n",
       "      <th>win</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>126</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>AUS</td>\n",
       "      <td>B+2</td>\n",
       "      <td>Ice Point</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>1115</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{B}</td>\n",
       "      <td>G</td>\n",
       "      <td>2018_C517</td>\n",
       "      <td>ALL FOR ST PAUL'S</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>PPG</td>\n",
       "      <td>M F Poon</td>\n",
       "      <td>93.0</td>\n",
       "      <td>23/24_324</td>\n",
       "      <td>9-7-3-46</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4130807.0</td>\n",
       "      <td>{\"R.B.\": 33}</td>\n",
       "      <td>6.018125</td>\n",
       "      <td>6.145556</td>\n",
       "      <td>39555603.0</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>3120000.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>61569309.0</td>\n",
       "      <td>53053464.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>C2</td>\n",
       "      <td>ST</td>\n",
       "      <td>13.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>93.0</td>\n",
       "      <td>100-80</td>\n",
       "      <td>{2018_C102,2023_J117,2020_E061,2019_D375}</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3853200.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Magnus</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>93.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>14079105.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>{\"SmT\": 21, \"AWT\": 6, \"TroR\": 6, \"Turf\": 1}</td>\n",
       "      <td>{\"H\": 33, \"B\": 1}</td>\n",
       "      <td>F C Lor</td>\n",
       "      <td>39.0</td>\n",
       "      <td>{\"Trotting\": 27, \"Gallop\": 6}</td>\n",
       "      <td>{\"Sha Tin\": 6, \"Conghua\": 27, \"Happy Valley\": 1}</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1053366.0</td>\n",
       "      <td>49795487.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Chestnut</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Show A Heart</td>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>1315</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>9</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>2021_G346</td>\n",
       "      <td>MIGHTY STRIDE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>PPG</td>\n",
       "      <td>C Lemaire</td>\n",
       "      <td>59.0</td>\n",
       "      <td>22/23_234</td>\n",
       "      <td>3-2-0-8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>17957399.0</td>\n",
       "      <td>{\"R.B.\": 8, \"A.T.\": 11}</td>\n",
       "      <td>5.793333</td>\n",
       "      <td>5.722500</td>\n",
       "      <td>26119853.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1080000.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35774456.0</td>\n",
       "      <td>35625418.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C4</td>\n",
       "      <td>ST</td>\n",
       "      <td>14.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>59.0</td>\n",
       "      <td>60-40</td>\n",
       "      <td>{2022_H059,2021_G088,2023_J223,2022_H184,2022_...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>111600.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Rich Enuff</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>3085050.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>{\"SmT\": 12, \"TroR\": 4, \"Turf\": 2, \"AWT\": 2}</td>\n",
       "      <td>{\"H\": 6}</td>\n",
       "      <td>P F Yiu</td>\n",
       "      <td>27.0</td>\n",
       "      <td>{\"Swimming\": 14, \"Trotting\": 16, \"Gallop\": 3}</td>\n",
       "      <td>{\"Sha Tin\": 5, \"Conghua\": 29}</td>\n",
       "      <td>2.3</td>\n",
       "      <td>11298623.0</td>\n",
       "      <td>31499192.0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      act_wt  age  age_re  b4_flow  b4_win    colour country of origin course  \\\n",
       "386      126  7.0     7.0     49.0    28.0       Bay               AUS    B+2   \n",
       "8519     132  4.0     2.0    124.0     2.6  Chestnut               AUS      A   \n",
       "\n",
       "        dam's sire        date  declarhorse_wt    dist  draw draw_counts  \\\n",
       "386      Ice Point  2024-01-07            1115  1600.0     1          {}   \n",
       "8519  Show A Heart  2022-12-11            1315  1200.0     9          {}   \n",
       "\n",
       "     gear_list going   horse_id         horse_name  horse_no  hv_rain  \\\n",
       "386        {B}     G  2018_C517  ALL FOR ST PAUL'S       4.0      0.0   \n",
       "8519       NaN     G  2021_G346      MIGHTY STRIDE       2.0      0.0   \n",
       "\n",
       "      hv_temp import type     jockey  last rating n_raceindex  \\\n",
       "386      20.0         PPG   M F Poon         93.0   23/24_324   \n",
       "8519     17.3         PPG  C Lemaire         59.0   22/23_234   \n",
       "\n",
       "     no. of 1-2-3-starts*  p_odds  p_odds_allo         p_trainer_counts  \\\n",
       "386              9-7-3-46     7.9    4130807.0             {\"R.B.\": 33}   \n",
       "8519              3-2-0-8     1.2   17957399.0  {\"R.B.\": 8, \"A.T.\": 11}   \n",
       "\n",
       "       pace_sp  pace_sp_shifted       place  placing  placing_shifted  \\\n",
       "386   6.018125         6.145556  39555603.0       13               11   \n",
       "8519  5.793333         5.722500  26119853.0        1                1   \n",
       "\n",
       "          prize  q_odds  qp_odds    quinella  quinella place  race raceclass  \\\n",
       "386   3120000.0    38.0     14.0  61569309.0      53053464.0  10.0        C2   \n",
       "8519  1080000.0     3.8      2.0  35774456.0      35625418.0   2.0        C4   \n",
       "\n",
       "      rc  rc_counts result_counts  rtg. rtg_race  \\\n",
       "386   ST       13.0            {}  93.0   100-80   \n",
       "8519  ST       14.0            {}  59.0    60-40   \n",
       "\n",
       "                                              same sire  same_sire_score  \\\n",
       "386           {2018_C102,2023_J117,2020_E061,2019_D375}            111.0   \n",
       "8519  {2022_H059,2021_G088,2023_J223,2022_H184,2022_...             56.0   \n",
       "\n",
       "      season stakes*      sex        sire  sire_score  st_rain  st_temp  \\\n",
       "386        3853200.0  Gelding      Magnus        61.0      0.0     19.4   \n",
       "8519        111600.0  Gelding  Rich Enuff        23.0      0.0     16.0   \n",
       "\n",
       "      start ofseason rating time_per_distance  total stakes* track  \\\n",
       "386                    93.0                {}     14079105.0  turf   \n",
       "8519                   59.0                {}      3085050.0  turf   \n",
       "\n",
       "                                     track_counts         train_gear  trainer  \\\n",
       "386   {\"SmT\": 21, \"AWT\": 6, \"TroR\": 6, \"Turf\": 1}  {\"H\": 33, \"B\": 1}  F C Lor   \n",
       "8519  {\"SmT\": 12, \"TroR\": 4, \"Turf\": 2, \"AWT\": 2}           {\"H\": 6}  P F Yiu   \n",
       "\n",
       "      trot_sum                                    type_counts  \\\n",
       "386       39.0                  {\"Trotting\": 27, \"Gallop\": 6}   \n",
       "8519      27.0  {\"Swimming\": 14, \"Trotting\": 16, \"Gallop\": 3}   \n",
       "\n",
       "                                          venue_counts  w_odds  w_odds_allo  \\\n",
       "386   {\"Sha Tin\": 6, \"Conghua\": 27, \"Happy Valley\": 1}    39.0    1053366.0   \n",
       "8519                     {\"Sha Tin\": 5, \"Conghua\": 29}     2.3   11298623.0   \n",
       "\n",
       "             win  year  \n",
       "386   49795487.0  2024  \n",
       "8519  31499192.0  2022  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "fdf.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792f7a81-ff6e-48fb-9011-4d488906d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3(row):\n",
    "    if float(row['p_odds']) < 1.6:\n",
    "        return 0\n",
    "    elif int(row['placing']) <= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def top_1(row):\n",
    "  if float(row['w_odds']) < 3:\n",
    "     return 0\n",
    "  elif int(row['placing'])==1:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f79a605-cd8b-458e-917a-791303181ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "import pandas as pd\n",
    "\n",
    "# # Prepare the data for encoding\n",
    "# gear_list_series = df['gear_list'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "def explode_cols(df, cols_to_convert):\n",
    "\n",
    "    for col in cols_to_convert:\n",
    "        # Check if the column exists in the dataframe\n",
    "        if col in df.columns:\n",
    "            # Normalize the data for the current column\n",
    "            expanded_data = json_normalize(df[col])\n",
    "            \n",
    "            # Replace NaN with 0\n",
    "            expanded_data.fillna(0, inplace=True)\n",
    "\n",
    "            # Rename columns of expanded_data to avoid name conflicts when joining back\n",
    "            expanded_data.columns = [f\"{col}_{sub_col}\" for sub_col in expanded_data.columns]\n",
    "\n",
    "            # Drop the original column and join the expanded data back to the main dataframe\n",
    "            df = df.drop(col, axis=1).join(expanded_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Perprocess train_gear cols\n",
    "def combine_and_clean_columns_train(df):\n",
    "    # 1. Drop columns ending with '-'\n",
    "    df = df.drop(columns=[col for col in df.columns if col.endswith('-')])\n",
    "\n",
    "    # 2. Combine columns with trailing numbers into their base columns\n",
    "    base_cols = [col.rsplit('_', 1)[0] for col in df.columns if col[-1].isdigit() and '_' in col]\n",
    "    for base_col in set(base_cols):\n",
    "        related_cols = [col for col in df.columns if col.startswith(base_col) and \n",
    "                        (col[-1].isdigit()) and col != base_col]\n",
    "        if related_cols:\n",
    "            if base_col not in df.columns:\n",
    "                df[base_col] = 0\n",
    "            for rc in related_cols:\n",
    "                df[base_col] += df[rc]\n",
    "                df.drop(columns=rc, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_train_gear(df):\n",
    "    \n",
    "    # Normalize the 'train_gear' column to separate columns\n",
    "    expanded_data = json_normalize(df['train_gear'])\n",
    "\n",
    "    # Replace NaN with 0\n",
    "    expanded_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Prefix the columns for clarity\n",
    "    expanded_data.columns = [f\"train_gear_{col}\" for col in expanded_data.columns]\n",
    "\n",
    "    # Clean and combine columns\n",
    "    cleaned_train_gear_df = combine_and_clean_columns_train(expanded_data)\n",
    "\n",
    "    # Drop the original 'train_gear' column and merge the new cleaned dataframe\n",
    "\n",
    "    df = pd.concat([df, cleaned_train_gear_df], axis=1)\n",
    "    df = df.drop('train_gear', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1831f90-8c3b-4063-b123-1c8947885301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def handle_gear_string(gear_string):\n",
    "    if gear_string == '{\"\"}':\n",
    "        return []\n",
    "    elif gear_string and isinstance(gear_string, str):\n",
    "        # Remove curly braces and split by comma\n",
    "        gears = gear_string.strip('{}').split(',')\n",
    "        return gears\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "        \n",
    "def combine_and_clean_columns_race(df):\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    # Create a set to keep track of base columns to combine\n",
    "    base_cols = set([col if '-' not in col and not col[-1].isdigit() else col[:-2] if col[-1].isdigit() and '-' in col else col[:-1] for col in cols])\n",
    "    \n",
    "    # Remove columns with '-'\n",
    "    for col in cols:\n",
    "        if '-' in col:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # Combine related columns\n",
    "    for base_col in base_cols:\n",
    "        related_cols = [col for col in df.columns if col.startswith(base_col) and (col == base_col or col[len(base_col)].isdigit())]\n",
    "        if len(related_cols) > 1:\n",
    "            df[base_col] = df[related_cols].sum(axis=1)\n",
    "            # Drop all but the base_col\n",
    "            for rc in related_cols:\n",
    "                if rc != base_col:\n",
    "                    df.drop(rc, axis=1, inplace=True)\n",
    "                \n",
    "    return df\n",
    "\n",
    "def label_encoded_info(df):\n",
    "\n",
    "    # Preprocess the 'gear_list' column using handle_gear_string function\n",
    "    df['gear_list'] = df['gear_list'].apply(handle_gear_string)\n",
    "\n",
    "    # Prepare the data for encoding gear_list\n",
    "    gear_encoded = df['gear_list'].apply(lambda x: ','.join(x) if isinstance(x, (list, set)) else '').str.get_dummies(sep=',')\n",
    "    \n",
    "    print(gear_encoded.columns)  # Printing the columns before cleaning\n",
    "    gear_encoded = combine_and_clean_columns_race(gear_encoded)\n",
    "    print(gear_encoded.columns)  # Printing the columns after cleaning\n",
    "    \n",
    "    # Drop the original 'gear_list' and 'same sire' columns and merge the new encoded dataframes\n",
    "    df = df.drop(['gear_list'], axis=1)\n",
    "    df = pd.concat([df, gear_encoded], axis=1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def process_same_sire_column(orig_df):\n",
    "    df = orig_df.copy()\n",
    "    # Define a function to clean each entry in the 'same sire' column\n",
    "    def clean_entry(entry):\n",
    "        # Convert to string\n",
    "        entry_str = str(entry)\n",
    "        # Remove unwanted characters\n",
    "        cleaned = entry_str.replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").strip()\n",
    "        # Split by comma, then strip leading/trailing spaces for each entry, and rejoin\n",
    "        cleaned_list = [item.strip() for item in cleaned.split(',')]\n",
    "        cleaned = ','.join(cleaned_list)\n",
    "        if not cleaned: # If empty after cleaning\n",
    "            return None\n",
    "        return cleaned\n",
    "\n",
    "    # Clean the 'same sire' column\n",
    "    df['cleaned_same_sire'] = df['same sire'].apply(clean_entry)\n",
    "\n",
    "    # # Drop rows where 'cleaned_same_sire' is None\n",
    "    # df = df[df['cleaned_same_sire'].notna()]\n",
    "\n",
    "    # # Get dummies\n",
    "    # same_sire_encoded = df['cleaned_same_sire'].str.get_dummies(sep=',')\n",
    "\n",
    "    # # Drop the original 'same sire' and temporary columns, and merge the new encoded dataframe\n",
    "    # df = df.drop(['same sire', 'cleaned_same_sire'], axis=1)\n",
    "    # df = pd.concat([df, same_sire_encoded], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f0408b-6907-4cc8-bf2f-88fceda07582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode_columns(df, columns):\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9900d604-d7d8-4c44-a70e-9b0ff85ea7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col(df):\n",
    "    # df = df.dropna(subset=['race_pla'])\n",
    "    cols_to_drop = ['age', 'b4_flow', 'b4_win', \n",
    "                    'date',   'gear_list',  'horse_name',  \n",
    "                    'n_raceindex', 'p_trainer_counts',  'same sire',   \n",
    "                     'year' ]\n",
    "\n",
    "    # Use the 'drop' method and specify axis=1 to drop columns\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664e96a3-246e-4b52-b491-4d85647b0eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the dataframe\n",
    "def process_dataframe(orig_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process the dataframe by applying a series of transformations\n",
    "    including exploding columns, combining and cleaning columns, and processing train gear data.\n",
    "    \"\"\"\n",
    "    df = orig_df.copy()\n",
    "\n",
    "    df['top3'] = df.apply(top_3, axis=1)\n",
    "    df['top1'] = df.apply(top_1, axis=1)\n",
    "    # Apply the transformations\n",
    "    cols_to_convert = ['draw_counts','p_trainer_counts', 'track_counts', 'result_counts','time_per_distance', \n",
    "                       'type_counts', 'venue_counts']\n",
    "    \n",
    "    df = explode_cols(df, cols_to_convert)\n",
    "\n",
    "    df = process_train_gear(df)\n",
    "    \n",
    "    df = label_encoded_info(df)\n",
    "    \n",
    "#     df = process_same_sire_column(df)\n",
    "    \n",
    "    # Drop cols\n",
    "    df = drop_col(df)\n",
    "    \n",
    "    float_covert = ['act_wt','age_re','declarhorse_wt', 'draw', 'hv_rain','hv_temp', \n",
    "                       'last rating',\n",
    "                     'p_odds', 'pace_sp_shifted', 'placing_shifted', \n",
    "                   'place','prize', 'quinella', 'quinella place', 'rc_counts',\n",
    "                    'rtg.','same_sire_score', 'season stakes*', 'sire_score', \n",
    "                    'st_temp', 'start ofseason rating',\n",
    "                    'st_rain','total stakes*', 'w_odds', 'win']\n",
    "    df[float_covert] = df[float_covert].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d79a8d-496f-44fb-bc60-43b5e9ca6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B', 'B-', 'B1', 'B2', 'BO', 'BO-', 'BO1', 'BO2', 'CP', 'CP-', 'CP1',\n",
      "       'CP2', 'E', 'E-', 'E1', 'E2', 'H', 'H-', 'H1', 'H2', 'P', 'P-', 'P1',\n",
      "       'P2', 'PC', 'PC-', 'PC1', 'PC2', 'SB', 'SB-', 'SB1', 'SR', 'SR-', 'SR1',\n",
      "       'SR2', 'TT', 'TT-', 'TT1', 'TT2', 'V', 'V-', 'V1', 'V2', 'VO', 'VO-',\n",
      "       'VO1', 'XB', 'XB-', 'XB1', 'XB2'],\n",
      "      dtype='object')\n",
      "Index(['B', 'BO', 'CP', 'E', 'H', 'P', 'PC', 'SB', 'SR', 'TT', 'V', 'VO',\n",
      "       'XB'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "label_df = process_dataframe(fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86990439-94b5-437c-810c-1af3112ee475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_df = label_df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b41103-dd25-4ed3-852f-04e41a744bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: ['horse_no', 'p_odds', 'p_odds_allo', 'q_odds', 'qp_odds', 'rtg_race', 'season stakes*', 'w_odds', 'w_odds_allo']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>BO</th>\n",
       "      <th>CP</th>\n",
       "      <th>E</th>\n",
       "      <th>H</th>\n",
       "      <th>P</th>\n",
       "      <th>PC</th>\n",
       "      <th>SB</th>\n",
       "      <th>SR</th>\n",
       "      <th>TT</th>\n",
       "      <th>V</th>\n",
       "      <th>VO</th>\n",
       "      <th>XB</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>age_re</th>\n",
       "      <th>colour</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>course</th>\n",
       "      <th>dam's sire</th>\n",
       "      <th>declarhorse_wt</th>\n",
       "      <th>dist</th>\n",
       "      <th>draw</th>\n",
       "      <th>going</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>hv_rain</th>\n",
       "      <th>hv_temp</th>\n",
       "      <th>import type</th>\n",
       "      <th>jockey</th>\n",
       "      <th>last rating</th>\n",
       "      <th>no. of 1-2-3-starts*</th>\n",
       "      <th>p_odds</th>\n",
       "      <th>p_odds_allo</th>\n",
       "      <th>pace_sp</th>\n",
       "      <th>pace_sp_shifted</th>\n",
       "      <th>place</th>\n",
       "      <th>placing</th>\n",
       "      <th>placing_shifted</th>\n",
       "      <th>prize</th>\n",
       "      <th>q_odds</th>\n",
       "      <th>qp_odds</th>\n",
       "      <th>quinella</th>\n",
       "      <th>quinella place</th>\n",
       "      <th>race</th>\n",
       "      <th>raceclass</th>\n",
       "      <th>rc</th>\n",
       "      <th>rc_counts</th>\n",
       "      <th>rtg.</th>\n",
       "      <th>rtg_race</th>\n",
       "      <th>same_sire_score</th>\n",
       "      <th>season stakes*</th>\n",
       "      <th>sex</th>\n",
       "      <th>sire</th>\n",
       "      <th>sire_score</th>\n",
       "      <th>st_rain</th>\n",
       "      <th>st_temp</th>\n",
       "      <th>start ofseason rating</th>\n",
       "      <th>top1</th>\n",
       "      <th>top3</th>\n",
       "      <th>total stakes*</th>\n",
       "      <th>track</th>\n",
       "      <th>trainer</th>\n",
       "      <th>trot_sum</th>\n",
       "      <th>w_odds</th>\n",
       "      <th>w_odds_allo</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{Bay,Brown}</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Rahy</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2020_E436</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19.3</td>\n",
       "      <td>PPG</td>\n",
       "      <td>J McDonald</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6-4-2-15</td>\n",
       "      <td>1.2</td>\n",
       "      <td>12435671.0</td>\n",
       "      <td>5.873125</td>\n",
       "      <td>5.895625</td>\n",
       "      <td>18088249.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>27186881.0</td>\n",
       "      <td>22913374.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>G1</td>\n",
       "      <td>ST</td>\n",
       "      <td>8.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285.0</td>\n",
       "      <td>14615250.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Deep Field</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41052975.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>P F Yiu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>12940499.0</td>\n",
       "      <td>29802362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Redoute's Choice</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2020_E269</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19.3</td>\n",
       "      <td>PPG</td>\n",
       "      <td>K Teetan</td>\n",
       "      <td>96.0</td>\n",
       "      <td>6-3-2-30</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1604603.0</td>\n",
       "      <td>5.915625</td>\n",
       "      <td>5.894286</td>\n",
       "      <td>18088249.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27186881.0</td>\n",
       "      <td>22913374.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>G1</td>\n",
       "      <td>ST</td>\n",
       "      <td>8.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1035050.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Written Tycoon</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11070175.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>D A Hayes</td>\n",
       "      <td>46.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>303543.0</td>\n",
       "      <td>29802362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chestnut</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Jeune</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2020_E058</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19.3</td>\n",
       "      <td>PP</td>\n",
       "      <td>K C Leung</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5-5-5-26</td>\n",
       "      <td>2.1</td>\n",
       "      <td>7106098.0</td>\n",
       "      <td>5.894375</td>\n",
       "      <td>5.916250</td>\n",
       "      <td>18088249.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>27186881.0</td>\n",
       "      <td>22913374.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>G1</td>\n",
       "      <td>ST</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4473500.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Sebring</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22744200.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>A S Cruz</td>\n",
       "      <td>108.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2048912.0</td>\n",
       "      <td>29802362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Tavistock</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2021_G435</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19.3</td>\n",
       "      <td>PP</td>\n",
       "      <td>C Y Ho</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4-0-1-15</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6783093.0</td>\n",
       "      <td>5.918125</td>\n",
       "      <td>6.106000</td>\n",
       "      <td>18088249.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>27186881.0</td>\n",
       "      <td>22913374.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>G1</td>\n",
       "      <td>ST</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6298000.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Fastnet Rock</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14733000.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>C Fownes</td>\n",
       "      <td>121.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2235177.0</td>\n",
       "      <td>29802362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>IRE</td>\n",
       "      <td>A</td>\n",
       "      <td>Fastnet Rock</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2020_E198</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19.3</td>\n",
       "      <td>PP</td>\n",
       "      <td>A Atzeni</td>\n",
       "      <td>121.0</td>\n",
       "      <td>4-3-2-28</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1507354.0</td>\n",
       "      <td>5.922500</td>\n",
       "      <td>6.123000</td>\n",
       "      <td>18088249.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27186881.0</td>\n",
       "      <td>22913374.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>G1</td>\n",
       "      <td>ST</td>\n",
       "      <td>8.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>781650.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Galileo</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48780298.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>D J Whyte</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>351242.0</td>\n",
       "      <td>29802362.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    B  BO  CP  E  H  P  PC  SB  SR  TT  V  VO  XB  act_wt  age_re  \\\n",
       "51  0   0   1  0  0  0   0   0   0   0  0   0   0   126.0     5.0   \n",
       "73  0   0   1  0  0  0   0   0   0   1  0   0   0   126.0     5.0   \n",
       "84  0   0   0  0  1  0   0   0   0   1  0   0   0   126.0     5.0   \n",
       "86  0   0   0  0  0  0   0   0   0   0  0   0   0   126.0     4.0   \n",
       "97  0   0   0  0  0  0   0   0   1   0  0   0   0   126.0     5.0   \n",
       "\n",
       "         colour country of origin course        dam's sire  declarhorse_wt  \\\n",
       "51  {Bay,Brown}               AUS      A              Rahy          1230.0   \n",
       "73          Bay               AUS      A  Redoute's Choice          1217.0   \n",
       "84     Chestnut               AUS      A             Jeune          1035.0   \n",
       "86          Bay               AUS      A         Tavistock          1127.0   \n",
       "97          Bay               IRE      A      Fastnet Rock          1160.0   \n",
       "\n",
       "      dist  draw going   horse_id  horse_no  hv_rain  hv_temp import type  \\\n",
       "51  1600.0   3.0     G  2020_E436       2.0     0.05     19.3         PPG   \n",
       "73  1600.0   4.0     G  2020_E269       8.0     0.05     19.3         PPG   \n",
       "84  1600.0   5.0     G  2020_E058       6.0     0.05     19.3          PP   \n",
       "86  1600.0   1.0     G  2021_G435       3.0     0.05     19.3          PP   \n",
       "97  1600.0   6.0     G  2020_E198       4.0     0.05     19.3          PP   \n",
       "\n",
       "        jockey  last rating no. of 1-2-3-starts*  p_odds  p_odds_allo  \\\n",
       "51  J McDonald        113.0             6-4-2-15     1.2   12435671.0   \n",
       "73    K Teetan         96.0             6-3-2-30     9.3    1604603.0   \n",
       "84   K C Leung        117.0             5-5-5-26     2.1    7106098.0   \n",
       "86      C Y Ho        117.0             4-0-1-15     2.2    6783093.0   \n",
       "97    A Atzeni        121.0             4-3-2-28     9.9    1507354.0   \n",
       "\n",
       "     pace_sp  pace_sp_shifted       place  placing  placing_shifted  \\\n",
       "51  5.873125         5.895625  18088249.0        1              2.0   \n",
       "73  5.915625         5.894286  18088249.0        5              6.0   \n",
       "84  5.894375         5.916250  18088249.0        3              5.0   \n",
       "86  5.918125         6.106000  18088249.0        6              4.0   \n",
       "97  5.922500         6.123000  18088249.0        7              8.0   \n",
       "\n",
       "         prize  q_odds  qp_odds    quinella  quinella place  race raceclass  \\\n",
       "51  13000000.0     3.2      1.6  27186881.0      22913374.0   7.0        G1   \n",
       "73  13000000.0    70.0     17.0  27186881.0      22913374.0   7.0        G1   \n",
       "84  13000000.0     7.7      3.2  27186881.0      22913374.0   7.0        G1   \n",
       "86  13000000.0     9.7      2.7  27186881.0      22913374.0   7.0        G1   \n",
       "97  13000000.0    83.0     19.0  27186881.0      22913374.0   7.0        G1   \n",
       "\n",
       "    rc  rc_counts   rtg. rtg_race  same_sire_score  season stakes*      sex  \\\n",
       "51  ST        8.0  113.0      NaN            285.0      14615250.0  Gelding   \n",
       "73  ST        8.0   96.0      NaN             87.0       1035050.0  Gelding   \n",
       "84  ST        8.0  117.0      NaN             79.0       4473500.0  Gelding   \n",
       "86  ST        8.0  117.0      NaN             92.0       6298000.0  Gelding   \n",
       "97  ST        8.0  121.0      NaN            -19.0        781650.0  Gelding   \n",
       "\n",
       "              sire  sire_score  st_rain  st_temp  start ofseason rating  top1  \\\n",
       "51      Deep Field        53.0      0.0     17.7                  113.0     0   \n",
       "73  Written Tycoon        33.0      0.0     17.7                   96.0     0   \n",
       "84         Sebring        49.0      0.0     17.7                  117.0     0   \n",
       "86    Fastnet Rock        16.0      0.0     17.7                  117.0     0   \n",
       "97         Galileo        21.0      0.0     17.7                  121.0     0   \n",
       "\n",
       "    top3  total stakes* track    trainer  trot_sum  w_odds  w_odds_allo  \\\n",
       "51     0     41052975.0  turf    P F Yiu     100.0     1.9   12940499.0   \n",
       "73     0     11070175.0  turf  D A Hayes      46.0    81.0     303543.0   \n",
       "84     1     22744200.0  turf   A S Cruz     108.0    12.0    2048912.0   \n",
       "86     0     14733000.0  turf   C Fownes     121.0    11.0    2235177.0   \n",
       "97     0     48780298.0  turf  D J Whyte      80.0    70.0     351242.0   \n",
       "\n",
       "           win  \n",
       "51  29802362.0  \n",
       "73  29802362.0  \n",
       "84  29802362.0  \n",
       "86  29802362.0  \n",
       "97  29802362.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Find columns that have NaN values\n",
    "nan_columns = sorted_df.columns[sorted_df.isna().any()].tolist()\n",
    "\n",
    "# Print the column names that contain NaN values\n",
    "print(\"Columns with NaN values:\", nan_columns)\n",
    "\n",
    "sorted_df[sorted_df['rtg_race'].isna()].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a939cbcf-bfaa-4a61-8b86-a017555ef993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pycaret.classification import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "# Define features and targets\n",
    "top1_df = sorted_df.drop(columns=['top3', 'horse_no'])\n",
    "top3_df = sorted_df.drop(columns=['top1', 'horse_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a56cd76-61bb-42f7-9c85-2cb8b3983372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = ['colour','country of origin','course', 'dist', 'draw',\n",
    "                        \"dam's sire\", 'going', 'jockey', 'import type', 'raceclass','rc', 'rtg_race', \n",
    "                        'sex', 'sire', 'track', \n",
    "                        'horse_id', 'trainer']\n",
    "\n",
    "ignore_features = ['qp_odds','q_odds', 'no. of 1-2-3-starts*', 'pace_sp', 'placing','quinella','quinella place']\n",
    "\n",
    "ordinal_features =  {'raceclass' : ['c5', 'c4', 'c3', 'c2', 'c1','other', 'g3', 'g1', 'g2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30551a10-38dc-43c1-86f1-de7fb9a77d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C4', 'C2', 'C3', 'C5', 'G1', 'G3', 'C1', 'G2', 'Other'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "sorted_df['raceclass'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28cabd-9181-4387-aba4-d9361af80b86",
   "metadata": {},
   "source": [
    "## Setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67bbcf51-7cd3-40cc-b8cf-c39719db82ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>BO</th>\n",
       "      <th>CP</th>\n",
       "      <th>E</th>\n",
       "      <th>H</th>\n",
       "      <th>P</th>\n",
       "      <th>PC</th>\n",
       "      <th>SB</th>\n",
       "      <th>SR</th>\n",
       "      <th>TT</th>\n",
       "      <th>V</th>\n",
       "      <th>VO</th>\n",
       "      <th>XB</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>age_re</th>\n",
       "      <th>colour</th>\n",
       "      <th>country of origin</th>\n",
       "      <th>course</th>\n",
       "      <th>dam's sire</th>\n",
       "      <th>declarhorse_wt</th>\n",
       "      <th>dist</th>\n",
       "      <th>draw</th>\n",
       "      <th>going</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>hv_rain</th>\n",
       "      <th>hv_temp</th>\n",
       "      <th>import type</th>\n",
       "      <th>jockey</th>\n",
       "      <th>last rating</th>\n",
       "      <th>no. of 1-2-3-starts*</th>\n",
       "      <th>p_odds</th>\n",
       "      <th>p_odds_allo</th>\n",
       "      <th>pace_sp</th>\n",
       "      <th>pace_sp_shifted</th>\n",
       "      <th>place</th>\n",
       "      <th>placing</th>\n",
       "      <th>placing_shifted</th>\n",
       "      <th>prize</th>\n",
       "      <th>q_odds</th>\n",
       "      <th>qp_odds</th>\n",
       "      <th>quinella</th>\n",
       "      <th>quinella place</th>\n",
       "      <th>race</th>\n",
       "      <th>raceclass</th>\n",
       "      <th>rc</th>\n",
       "      <th>rc_counts</th>\n",
       "      <th>rtg.</th>\n",
       "      <th>rtg_race</th>\n",
       "      <th>same_sire_score</th>\n",
       "      <th>season stakes*</th>\n",
       "      <th>sex</th>\n",
       "      <th>sire</th>\n",
       "      <th>sire_score</th>\n",
       "      <th>st_rain</th>\n",
       "      <th>st_temp</th>\n",
       "      <th>start ofseason rating</th>\n",
       "      <th>top1</th>\n",
       "      <th>top3</th>\n",
       "      <th>total stakes*</th>\n",
       "      <th>track</th>\n",
       "      <th>trainer</th>\n",
       "      <th>trot_sum</th>\n",
       "      <th>w_odds</th>\n",
       "      <th>w_odds_allo</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8615</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Grey</td>\n",
       "      <td>AUS</td>\n",
       "      <td>awt</td>\n",
       "      <td>Danerich</td>\n",
       "      <td>1183.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2020_E320</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.1</td>\n",
       "      <td>PP</td>\n",
       "      <td>L Hewitson</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2-1-2-25</td>\n",
       "      <td>1.4</td>\n",
       "      <td>13181415.0</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>5.815000</td>\n",
       "      <td>22368462.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>30467930.0</td>\n",
       "      <td>29831564.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C5</td>\n",
       "      <td>ST</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40-0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>The Factor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1605250.0</td>\n",
       "      <td>awt</td>\n",
       "      <td>D A Hayes</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5187513.0</td>\n",
       "      <td>23265212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Fastnet Rock</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2022_H170</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>28.8</td>\n",
       "      <td>PP</td>\n",
       "      <td>K C Leung</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1-2-2-9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>9884890.0</td>\n",
       "      <td>5.832500</td>\n",
       "      <td>5.823333</td>\n",
       "      <td>26359708.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1725000.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>44296778.0</td>\n",
       "      <td>38540731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>C3</td>\n",
       "      <td>ST</td>\n",
       "      <td>12.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80-60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1639950.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Dream Ahead</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1803825.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>T P Yung</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3041607.0</td>\n",
       "      <td>40554762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>AUS</td>\n",
       "      <td>B+2</td>\n",
       "      <td>Bel Esprit</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2021_G003</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.2</td>\n",
       "      <td>PPG</td>\n",
       "      <td>K Teetan</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2-0-1-18</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8774986.0</td>\n",
       "      <td>5.857500</td>\n",
       "      <td>5.866667</td>\n",
       "      <td>24463598.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1040000.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>32216823.0</td>\n",
       "      <td>30833924.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C4</td>\n",
       "      <td>ST</td>\n",
       "      <td>14.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>60-40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222750.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Akeed Mofeed</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1634550.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>W Y So</td>\n",
       "      <td>54.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2706519.0</td>\n",
       "      <td>27557288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Bay</td>\n",
       "      <td>NZ</td>\n",
       "      <td>A</td>\n",
       "      <td>Volksraad</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2020_E296</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>28.7</td>\n",
       "      <td>PPG</td>\n",
       "      <td>R Maia</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1-1-1-27</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6166735.0</td>\n",
       "      <td>6.149091</td>\n",
       "      <td>6.206111</td>\n",
       "      <td>18687075.0</td>\n",
       "      <td>6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>780000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>24518862.0</td>\n",
       "      <td>24100744.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>C5</td>\n",
       "      <td>HV</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40-0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>48125.0</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>Rip Van Winkle</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>866175.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>K W Lui</td>\n",
       "      <td>55.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1798462.0</td>\n",
       "      <td>19401589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{Bay,Brown}</td>\n",
       "      <td>AUS</td>\n",
       "      <td>A</td>\n",
       "      <td>Fastnet Rock</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>G</td>\n",
       "      <td>2019_D472</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>29.5</td>\n",
       "      <td>PPG</td>\n",
       "      <td>M Chadwick</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2-1-3-35</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3531411.0</td>\n",
       "      <td>6.089375</td>\n",
       "      <td>5.987143</td>\n",
       "      <td>20546392.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29082467.0</td>\n",
       "      <td>27404246.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C5</td>\n",
       "      <td>ST</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40-0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>More Than Ready</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>29.1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2172000.0</td>\n",
       "      <td>turf</td>\n",
       "      <td>F C Lor</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1075081.0</td>\n",
       "      <td>20850048.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B  BO  CP  E  H  P  PC  SB  SR  TT  V  VO  XB  act_wt  age_re  \\\n",
       "8615   1   0   0  0  0  0   0   0   0   1  0   0   0   132.0     3.0   \n",
       "4512   0   0   0  0  0  0   0   0   0   1  0   0   0   122.0     2.0   \n",
       "12859  0   0   0  0  0  0   0   0   0   1  0   0   0   125.0     2.0   \n",
       "10392  0   0   0  0  1  0   0   0   1   0  0   0   0   131.0     3.0   \n",
       "3827   0   0   0  0  0  0   0   0   0   1  1   0   0   132.0     5.0   \n",
       "\n",
       "            colour country of origin course    dam's sire  declarhorse_wt  \\\n",
       "8615          Grey               AUS    awt      Danerich          1183.0   \n",
       "4512           Bay               AUS      A  Fastnet Rock          1229.0   \n",
       "12859          Bay               AUS    B+2    Bel Esprit          1155.0   \n",
       "10392          Bay                NZ      A     Volksraad          1024.0   \n",
       "3827   {Bay,Brown}               AUS      A  Fastnet Rock          1103.0   \n",
       "\n",
       "         dist  draw going   horse_id  horse_no  hv_rain  hv_temp import type  \\\n",
       "8615   1200.0   6.0     G  2020_E320       5.0     0.00     22.1          PP   \n",
       "4512   1200.0   1.0     G  2022_H170       7.0     0.05     28.8          PP   \n",
       "12859  1200.0  11.0     G  2021_G003      10.0     0.00     17.2         PPG   \n",
       "10392  1650.0   4.0     G  2020_E296       3.0     0.50     28.7         PPG   \n",
       "3827   1600.0   3.0     G  2019_D472       4.0    13.00     29.5         PPG   \n",
       "\n",
       "           jockey  last rating no. of 1-2-3-starts*  p_odds  p_odds_allo  \\\n",
       "8615   L Hewitson         37.0             2-1-2-25     1.4   13181415.0   \n",
       "4512    K C Leung         64.0              1-2-2-9     2.2    9884890.0   \n",
       "12859    K Teetan         52.0             2-0-1-18     2.3    8774986.0   \n",
       "10392      R Maia         36.0             1-1-1-27     2.5    6166735.0   \n",
       "3827   M Chadwick         36.0             2-1-3-35     4.8    3531411.0   \n",
       "\n",
       "        pace_sp  pace_sp_shifted       place  placing  placing_shifted  \\\n",
       "8615   5.733333         5.815000  22368462.0        1              3.0   \n",
       "4512   5.832500         5.823333  26359708.0        6              4.0   \n",
       "12859  5.857500         5.866667  24463598.0        5              5.0   \n",
       "10392  6.149091         6.206111  18687075.0        6             11.0   \n",
       "3827   6.089375         5.987143  20546392.0        9              7.0   \n",
       "\n",
       "           prize  q_odds  qp_odds    quinella  quinella place  race raceclass  \\\n",
       "8615    810000.0     5.7      2.5  30467930.0      29831564.0   1.0        C5   \n",
       "4512   1725000.0    11.0      4.1  44296778.0      38540731.0  10.0        C3   \n",
       "12859  1040000.0     8.5      4.2  32216823.0      30833924.0   2.0        C4   \n",
       "10392   780000.0    15.0      5.4  24518862.0      24100744.0   3.0        C5   \n",
       "3827    810000.0    36.0     13.0  29082467.0      27404246.0   2.0        C5   \n",
       "\n",
       "       rc  rc_counts  rtg. rtg_race  same_sire_score  season stakes*      sex  \\\n",
       "8615   ST       12.0  37.0     40-0             30.0             NaN  Gelding   \n",
       "4512   ST       12.0  64.0    80-60              0.0       1639950.0  Gelding   \n",
       "12859  ST       14.0  52.0    60-40              0.0        222750.0  Gelding   \n",
       "10392  HV       11.0  36.0     40-0             30.0         48125.0  Gelding   \n",
       "3827   ST       14.0  36.0     40-0             43.0             NaN  Gelding   \n",
       "\n",
       "                  sire  sire_score  st_rain  st_temp  start ofseason rating  \\\n",
       "8615        The Factor         0.0      0.0     20.1                   37.0   \n",
       "4512       Dream Ahead        14.0      0.0     28.4                   64.0   \n",
       "12859     Akeed Mofeed        -1.0      0.0     16.8                   52.0   \n",
       "10392   Rip Van Winkle       -12.0      6.5     28.5                   36.0   \n",
       "3827   More Than Ready        -7.0      6.5     29.1                   36.0   \n",
       "\n",
       "       top1  top3  total stakes* track    trainer  trot_sum  w_odds  \\\n",
       "8615      1     0      1605250.0   awt  D A Hayes      51.0     3.7   \n",
       "4512      0     0      1803825.0  turf   T P Yung      38.0    11.0   \n",
       "12859     0     0      1634550.0  turf     W Y So      54.0     8.4   \n",
       "10392     0     0       866175.0  turf    K W Lui      55.0     8.9   \n",
       "3827      0     0      2172000.0  turf    F C Lor      19.0    16.0   \n",
       "\n",
       "       w_odds_allo         win  \n",
       "8615     5187513.0  23265212.0  \n",
       "4512     3041607.0  40554762.0  \n",
       "12859    2706519.0  27557288.0  \n",
       "10392    1798462.0  19401589.0  \n",
       "3827     1075081.0  20850048.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "sorted_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5924a32-5b27-4761-9d89-f0c8a8c8629d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7110\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 5.990018\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6988\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 6966925.282951\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6988\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 27.983189\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6988\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 2886752.818375\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 6678, number of negative: 4220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7080\n",
      "[LightGBM] [Info] Number of data points in the train set: 10898, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.612773 -> initscore=0.458983\n",
      "[LightGBM] [Info] Start training from score 0.458983\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6412\n",
      "[LightGBM] [Info] Number of data points in the train set: 8900, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 1067424.138315\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7116\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 5.990018\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 6966925.282951\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 27.983189\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 2886752.818375\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 6678, number of negative: 4220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7084\n",
      "[LightGBM] [Info] Number of data points in the train set: 10898, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.612773 -> initscore=0.458983\n",
      "[LightGBM] [Info] Start training from score 0.458983\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6412\n",
      "[LightGBM] [Info] Number of data points in the train set: 8900, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 1067424.138315\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7116\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 5.990018\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 6966925.282951\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 27.983189\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 2886752.818375\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 6678, number of negative: 4220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7084\n",
      "[LightGBM] [Info] Number of data points in the train set: 10898, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.612773 -> initscore=0.458983\n",
      "[LightGBM] [Info] Start training from score 0.458983\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6412\n",
      "[LightGBM] [Info] Number of data points in the train set: 8900, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 1067424.138315\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7116\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 5.990018\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 6966925.282951\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 27.983189\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 2886752.818375\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 6678, number of negative: 4220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7084\n",
      "[LightGBM] [Info] Number of data points in the train set: 10898, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.612773 -> initscore=0.458983\n",
      "[LightGBM] [Info] Start training from score 0.458983\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6412\n",
      "[LightGBM] [Info] Number of data points in the train set: 8900, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 1067424.138315\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7116\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 5.990018\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 6966925.282951\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 27.983189\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 11320, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 2886752.818375\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 6678, number of negative: 4220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7084\n",
      "[LightGBM] [Info] Number of data points in the train set: 10898, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.612773 -> initscore=0.458983\n",
      "[LightGBM] [Info] Start training from score 0.458983\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6411\n",
      "[LightGBM] [Info] Number of data points in the train set: 8900, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 1067424.138315\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7c30e_row12_col1, #T_7c30e_row19_col1, #T_7c30e_row21_col1, #T_7c30e_row23_col1, #T_7c30e_row25_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7c30e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7c30e_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_7c30e_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7c30e_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_7c30e_row0_col1\" class=\"data row0 col1\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7c30e_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_7c30e_row1_col1\" class=\"data row1 col1\" >top3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7c30e_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_7c30e_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7c30e_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_7c30e_row3_col1\" class=\"data row3 col1\" >(16186, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7c30e_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_7c30e_row4_col1\" class=\"data row4 col1\" >(15619, 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7c30e_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_7c30e_row5_col1\" class=\"data row5 col1\" >(10763, 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7c30e_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_7c30e_row6_col1\" class=\"data row6 col1\" >(4856, 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7c30e_row7_col0\" class=\"data row7 col0\" >Ignore features</td>\n",
       "      <td id=\"T_7c30e_row7_col1\" class=\"data row7 col1\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7c30e_row8_col0\" class=\"data row8 col0\" >Ordinal features</td>\n",
       "      <td id=\"T_7c30e_row8_col1\" class=\"data row8 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7c30e_row9_col0\" class=\"data row9 col0\" >Numeric features</td>\n",
       "      <td id=\"T_7c30e_row9_col1\" class=\"data row9 col1\" >39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7c30e_row10_col0\" class=\"data row10 col0\" >Categorical features</td>\n",
       "      <td id=\"T_7c30e_row10_col1\" class=\"data row10 col1\" >17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_7c30e_row11_col0\" class=\"data row11 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_7c30e_row11_col1\" class=\"data row11 col1\" >24.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_7c30e_row12_col0\" class=\"data row12 col0\" >Preprocess</td>\n",
       "      <td id=\"T_7c30e_row12_col1\" class=\"data row12 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_7c30e_row13_col0\" class=\"data row13 col0\" >Imputation type</td>\n",
       "      <td id=\"T_7c30e_row13_col1\" class=\"data row13 col1\" >iterative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_7c30e_row14_col0\" class=\"data row14 col0\" >Iterative imputation iterations</td>\n",
       "      <td id=\"T_7c30e_row14_col1\" class=\"data row14 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_7c30e_row15_col0\" class=\"data row15 col0\" >Numeric iterative imputer</td>\n",
       "      <td id=\"T_7c30e_row15_col1\" class=\"data row15 col1\" >lightgbm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_7c30e_row16_col0\" class=\"data row16 col0\" >Categorical iterative imputer</td>\n",
       "      <td id=\"T_7c30e_row16_col1\" class=\"data row16 col1\" >lightgbm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_7c30e_row17_col0\" class=\"data row17 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_7c30e_row17_col1\" class=\"data row17 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_7c30e_row18_col0\" class=\"data row18 col0\" >Encoding method</td>\n",
       "      <td id=\"T_7c30e_row18_col1\" class=\"data row18 col1\" >CatBoostEncoder(a=1, cols=None, drop_invariant=False, handle_missing='value',\n",
       "                handle_unknown='value', random_state=None, return_df=True,\n",
       "                sigma=None, verbose=0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_7c30e_row19_col0\" class=\"data row19 col0\" >Remove multicollinearity</td>\n",
       "      <td id=\"T_7c30e_row19_col1\" class=\"data row19 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_7c30e_row20_col0\" class=\"data row20 col0\" >Multicollinearity threshold</td>\n",
       "      <td id=\"T_7c30e_row20_col1\" class=\"data row20 col1\" >0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_7c30e_row21_col0\" class=\"data row21 col0\" >Remove outliers</td>\n",
       "      <td id=\"T_7c30e_row21_col1\" class=\"data row21 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_7c30e_row22_col0\" class=\"data row22 col0\" >Outliers threshold</td>\n",
       "      <td id=\"T_7c30e_row22_col1\" class=\"data row22 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_7c30e_row23_col0\" class=\"data row23 col0\" >Transformation</td>\n",
       "      <td id=\"T_7c30e_row23_col1\" class=\"data row23 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_7c30e_row24_col0\" class=\"data row24 col0\" >Transformation method</td>\n",
       "      <td id=\"T_7c30e_row24_col1\" class=\"data row24 col1\" >yeo-johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_7c30e_row25_col0\" class=\"data row25 col0\" >Normalize</td>\n",
       "      <td id=\"T_7c30e_row25_col1\" class=\"data row25 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_7c30e_row26_col0\" class=\"data row26 col0\" >Normalize method</td>\n",
       "      <td id=\"T_7c30e_row26_col1\" class=\"data row26 col1\" >zscore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_7c30e_row27_col0\" class=\"data row27 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_7c30e_row27_col1\" class=\"data row27 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_7c30e_row28_col0\" class=\"data row28 col0\" >Fold Number</td>\n",
       "      <td id=\"T_7c30e_row28_col1\" class=\"data row28 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_7c30e_row29_col0\" class=\"data row29 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_7c30e_row29_col1\" class=\"data row29 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_7c30e_row30_col0\" class=\"data row30 col0\" >Use GPU</td>\n",
       "      <td id=\"T_7c30e_row30_col1\" class=\"data row30 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_7c30e_row31_col0\" class=\"data row31 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_7c30e_row31_col1\" class=\"data row31 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_7c30e_row32_col0\" class=\"data row32 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_7c30e_row32_col1\" class=\"data row32 col1\" >Top3 predict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c30e_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_7c30e_row33_col0\" class=\"data row33 col0\" >USI</td>\n",
       "      <td id=\"T_7c30e_row33_col1\" class=\"data row33 col1\" >59fd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d48718ee60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from category_encoders import CatBoostEncoder\n",
    "from pycaret.classification import *\n",
    "\n",
    "encoder = CatBoostEncoder()\n",
    "\n",
    "t3_model = setup(data = top3_df, target = 'top3', experiment_name = 'Top3 predict', train_size=0.7,\n",
    "                 # fix_imbalance = True,fix_imbalance_method = 'SMOTEENN',\n",
    "                 remove_outliers = True,\n",
    "                 imputation_type = 'iterative',\n",
    "                 numeric_imputation = 'mean',\n",
    "                 categorical_imputation = 'mode',\n",
    "                 # numeric_iterative_imputer = 'rf',\n",
    "                 # categorical_iterative_imputer = 'rf',\n",
    "                 #use_gpu = True,\n",
    "                 # fold = 20,\n",
    "                 remove_multicollinearity = True,\n",
    "                 multicollinearity_threshold = 0.95,\n",
    "                 # feature_selection = True, \n",
    "                 # feature_selection_estimator = 'gbc', feature_selection_method = 'classic',\n",
    "                 # n_features_to_select = 0.6,\n",
    "                 # pca = True,\n",
    "                 # pca_method = 'linear',\n",
    "                 # pca_components = 0.95,\n",
    "                 # max_encoding_ohe = 10,\n",
    "                 categorical_features = categorical_features,\n",
    "                 encoding_method = encoder,\n",
    "                 ignore_features = ignore_features,\n",
    "                 ordinal_features = ordinal_features,\n",
    "                 # numeric_features = numeric_features,\n",
    "                 transformation = True,\n",
    "                 data_split_stratify = True, \n",
    "                 normalize = True,\n",
    "                 session_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39b67943-5656-4b62-b602-8aa04dd7f5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>BO</th>\n",
       "      <th>CP</th>\n",
       "      <th>E</th>\n",
       "      <th>H</th>\n",
       "      <th>P</th>\n",
       "      <th>PC</th>\n",
       "      <th>SB</th>\n",
       "      <th>SR</th>\n",
       "      <th>TT</th>\n",
       "      <th>V</th>\n",
       "      <th>VO</th>\n",
       "      <th>XB</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>age_re</th>\n",
       "      <th>colour_Brown</th>\n",
       "      <th>colour_Grey</th>\n",
       "      <th>colour_Bay</th>\n",
       "      <th>colour_Chestnut</th>\n",
       "      <th>colour_BayBrown</th>\n",
       "      <th>colour_Dark BayBrown</th>\n",
       "      <th>colour_BrownGrey</th>\n",
       "      <th>colour_Dark Bay</th>\n",
       "      <th>colour_Roan</th>\n",
       "      <th>colour_Black</th>\n",
       "      <th>colour_BayGrey</th>\n",
       "      <th>country of origin_AUS</th>\n",
       "      <th>country of origin_NZ</th>\n",
       "      <th>country of origin_FR</th>\n",
       "      <th>country of origin_IRE</th>\n",
       "      <th>country of origin_USA</th>\n",
       "      <th>country of origin_CHI</th>\n",
       "      <th>country of origin_GB</th>\n",
       "      <th>country of origin_SAF</th>\n",
       "      <th>country of origin_GER</th>\n",
       "      <th>country of origin_JPN</th>\n",
       "      <th>country of origin_BRZ</th>\n",
       "      <th>country of origin_ARG</th>\n",
       "      <th>country of origin_ITY</th>\n",
       "      <th>course_C+3</th>\n",
       "      <th>course_C</th>\n",
       "      <th>course_A+3</th>\n",
       "      <th>course_B</th>\n",
       "      <th>course_A</th>\n",
       "      <th>course_awt</th>\n",
       "      <th>course_B+2</th>\n",
       "      <th>dam's sire</th>\n",
       "      <th>declarhorse_wt</th>\n",
       "      <th>dist_1200.0</th>\n",
       "      <th>dist_1400.0</th>\n",
       "      <th>dist_1650.0</th>\n",
       "      <th>dist_1600.0</th>\n",
       "      <th>dist_2000.0</th>\n",
       "      <th>dist_1000.0</th>\n",
       "      <th>dist_1800.0</th>\n",
       "      <th>dist_2400.0</th>\n",
       "      <th>dist_2200.0</th>\n",
       "      <th>draw_3.0</th>\n",
       "      <th>draw_11.0</th>\n",
       "      <th>draw_9.0</th>\n",
       "      <th>draw_2.0</th>\n",
       "      <th>draw_10.0</th>\n",
       "      <th>draw_14.0</th>\n",
       "      <th>draw_12.0</th>\n",
       "      <th>draw_1.0</th>\n",
       "      <th>draw_7.0</th>\n",
       "      <th>draw_4.0</th>\n",
       "      <th>draw_8.0</th>\n",
       "      <th>draw_6.0</th>\n",
       "      <th>draw_5.0</th>\n",
       "      <th>draw_13.0</th>\n",
       "      <th>going_G</th>\n",
       "      <th>going_GF</th>\n",
       "      <th>going_WS</th>\n",
       "      <th>going_GY</th>\n",
       "      <th>going_S</th>\n",
       "      <th>going_FT</th>\n",
       "      <th>going_Y</th>\n",
       "      <th>going_H</th>\n",
       "      <th>going_YS</th>\n",
       "      <th>going_WF</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>hv_rain</th>\n",
       "      <th>import type_ISG</th>\n",
       "      <th>import type_PPG</th>\n",
       "      <th>import type_PP</th>\n",
       "      <th>jockey</th>\n",
       "      <th>last rating</th>\n",
       "      <th>p_odds_allo</th>\n",
       "      <th>pace_sp_shifted</th>\n",
       "      <th>place</th>\n",
       "      <th>placing_shifted</th>\n",
       "      <th>prize</th>\n",
       "      <th>race</th>\n",
       "      <th>raceclass_-1.0</th>\n",
       "      <th>rc</th>\n",
       "      <th>rc_counts</th>\n",
       "      <th>rtg_race_80-60</th>\n",
       "      <th>rtg_race_60-40</th>\n",
       "      <th>rtg_race_100-80</th>\n",
       "      <th>rtg_race_40-0</th>\n",
       "      <th>rtg_race_105-80</th>\n",
       "      <th>rtg_race_100-75</th>\n",
       "      <th>rtg_race_60-35</th>\n",
       "      <th>rtg_race_115-90</th>\n",
       "      <th>rtg_race_100-70</th>\n",
       "      <th>rtg_race_80-55</th>\n",
       "      <th>rtg_race_90-65</th>\n",
       "      <th>rtg_race_85-60</th>\n",
       "      <th>rtg_race_110-85</th>\n",
       "      <th>rtg_race_90-70</th>\n",
       "      <th>rtg_race_95-75</th>\n",
       "      <th>rtg_race_95+</th>\n",
       "      <th>rtg_race_90+</th>\n",
       "      <th>rtg_race_105-75</th>\n",
       "      <th>rtg_race_110-80</th>\n",
       "      <th>same_sire_score</th>\n",
       "      <th>season stakes*</th>\n",
       "      <th>sex_Gelding</th>\n",
       "      <th>sex_Colt</th>\n",
       "      <th>sex_Horse</th>\n",
       "      <th>sex_Mare</th>\n",
       "      <th>sex_Filly</th>\n",
       "      <th>sire</th>\n",
       "      <th>sire_score</th>\n",
       "      <th>st_rain</th>\n",
       "      <th>st_temp</th>\n",
       "      <th>total stakes*</th>\n",
       "      <th>track</th>\n",
       "      <th>trainer</th>\n",
       "      <th>trot_sum</th>\n",
       "      <th>w_odds</th>\n",
       "      <th>w_odds_allo</th>\n",
       "      <th>win</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.0</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "      <td>15619.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.005468</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>-0.006992</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>-0.007166</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>-0.003288</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>-0.002692</td>\n",
       "      <td>-0.001700</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>-0.009141</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.009944</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>-0.003224</td>\n",
       "      <td>0.009984</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>-0.005565</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>-0.005995</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>-0.003559</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>-0.003982</td>\n",
       "      <td>-0.040260</td>\n",
       "      <td>-0.003702</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>-0.006783</td>\n",
       "      <td>-0.001702</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>-0.003237</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>-0.003123</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>-0.001888</td>\n",
       "      <td>-0.004599</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>-0.011833</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>-0.003522</td>\n",
       "      <td>-0.006983</td>\n",
       "      <td>0.012069</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>-0.001918</td>\n",
       "      <td>-0.014963</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>0.010242</td>\n",
       "      <td>-0.006236</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>-0.039693</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>-0.010410</td>\n",
       "      <td>-0.003924</td>\n",
       "      <td>-0.008286</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>-0.008974</td>\n",
       "      <td>-0.009122</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.025456</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>-0.006550</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.030216</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>0.018706</td>\n",
       "      <td>-0.007637</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>-0.001918</td>\n",
       "      <td>-0.002673</td>\n",
       "      <td>-0.021003</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>-0.008984</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>-0.066382</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>-0.009746</td>\n",
       "      <td>-0.007215</td>\n",
       "      <td>0.216915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.997518</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.006782</td>\n",
       "      <td>1.006787</td>\n",
       "      <td>1.003707</td>\n",
       "      <td>0.978672</td>\n",
       "      <td>1.018272</td>\n",
       "      <td>1.016706</td>\n",
       "      <td>1.004821</td>\n",
       "      <td>0.999767</td>\n",
       "      <td>0.996062</td>\n",
       "      <td>0.830169</td>\n",
       "      <td>1.012807</td>\n",
       "      <td>0.994102</td>\n",
       "      <td>1.003374</td>\n",
       "      <td>0.992132</td>\n",
       "      <td>1.026750</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>1.006944</td>\n",
       "      <td>1.008379</td>\n",
       "      <td>0.965643</td>\n",
       "      <td>0.985883</td>\n",
       "      <td>0.984476</td>\n",
       "      <td>0.877779</td>\n",
       "      <td>1.140781</td>\n",
       "      <td>0.830277</td>\n",
       "      <td>1.000206</td>\n",
       "      <td>0.998188</td>\n",
       "      <td>1.021297</td>\n",
       "      <td>0.996253</td>\n",
       "      <td>1.040016</td>\n",
       "      <td>0.971859</td>\n",
       "      <td>1.020420</td>\n",
       "      <td>1.000442</td>\n",
       "      <td>1.006154</td>\n",
       "      <td>0.936648</td>\n",
       "      <td>1.045582</td>\n",
       "      <td>1.055024</td>\n",
       "      <td>0.830193</td>\n",
       "      <td>0.998160</td>\n",
       "      <td>1.001279</td>\n",
       "      <td>1.007412</td>\n",
       "      <td>1.001580</td>\n",
       "      <td>0.997829</td>\n",
       "      <td>1.008002</td>\n",
       "      <td>0.993462</td>\n",
       "      <td>1.064525</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>1.002608</td>\n",
       "      <td>1.012389</td>\n",
       "      <td>1.023156</td>\n",
       "      <td>1.000601</td>\n",
       "      <td>0.988004</td>\n",
       "      <td>0.982281</td>\n",
       "      <td>1.031389</td>\n",
       "      <td>1.015945</td>\n",
       "      <td>1.000487</td>\n",
       "      <td>1.004664</td>\n",
       "      <td>1.003265</td>\n",
       "      <td>0.994809</td>\n",
       "      <td>0.980361</td>\n",
       "      <td>0.994570</td>\n",
       "      <td>1.008337</td>\n",
       "      <td>0.993777</td>\n",
       "      <td>1.001953</td>\n",
       "      <td>0.997182</td>\n",
       "      <td>0.993008</td>\n",
       "      <td>0.991554</td>\n",
       "      <td>1.020097</td>\n",
       "      <td>1.007296</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>1.076061</td>\n",
       "      <td>1.037860</td>\n",
       "      <td>0.978477</td>\n",
       "      <td>0.919799</td>\n",
       "      <td>1.047230</td>\n",
       "      <td>0.928250</td>\n",
       "      <td>1.312222</td>\n",
       "      <td>0.958591</td>\n",
       "      <td>1.023765</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>1.020800</td>\n",
       "      <td>1.000457</td>\n",
       "      <td>1.000338</td>\n",
       "      <td>1.062729</td>\n",
       "      <td>1.021582</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>1.009228</td>\n",
       "      <td>1.004798</td>\n",
       "      <td>1.014766</td>\n",
       "      <td>1.001574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998287</td>\n",
       "      <td>1.014762</td>\n",
       "      <td>0.996537</td>\n",
       "      <td>0.999022</td>\n",
       "      <td>1.006361</td>\n",
       "      <td>1.010584</td>\n",
       "      <td>1.061083</td>\n",
       "      <td>1.085194</td>\n",
       "      <td>1.042719</td>\n",
       "      <td>1.065276</td>\n",
       "      <td>1.002643</td>\n",
       "      <td>1.088490</td>\n",
       "      <td>0.896757</td>\n",
       "      <td>1.021179</td>\n",
       "      <td>1.022416</td>\n",
       "      <td>0.996183</td>\n",
       "      <td>1.012715</td>\n",
       "      <td>0.941366</td>\n",
       "      <td>1.016700</td>\n",
       "      <td>0.992218</td>\n",
       "      <td>2.033136</td>\n",
       "      <td>1.002371</td>\n",
       "      <td>1.068798</td>\n",
       "      <td>1.057065</td>\n",
       "      <td>1.063938</td>\n",
       "      <td>1.086722</td>\n",
       "      <td>0.958591</td>\n",
       "      <td>0.928154</td>\n",
       "      <td>1.064995</td>\n",
       "      <td>1.019167</td>\n",
       "      <td>1.002844</td>\n",
       "      <td>1.008005</td>\n",
       "      <td>1.036953</td>\n",
       "      <td>1.008002</td>\n",
       "      <td>1.083616</td>\n",
       "      <td>1.000158</td>\n",
       "      <td>1.005601</td>\n",
       "      <td>1.005343</td>\n",
       "      <td>1.007724</td>\n",
       "      <td>0.412158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.106630</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>-0.959475</td>\n",
       "      <td>-0.224630</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>-3.050586</td>\n",
       "      <td>-2.925155</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>-1.339182</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-1.045151</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>-2.633267</td>\n",
       "      <td>-3.389061</td>\n",
       "      <td>-0.824390</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.264350</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.281450</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.301710</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>-1.799742</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-1.991612</td>\n",
       "      <td>-0.700755</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>-1.073793</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>-3.318977</td>\n",
       "      <td>-4.590758</td>\n",
       "      <td>-2.554729</td>\n",
       "      <td>-3.423721</td>\n",
       "      <td>-4.059768</td>\n",
       "      <td>-1.863847</td>\n",
       "      <td>-2.528376</td>\n",
       "      <td>-1.767609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.266555</td>\n",
       "      <td>-2.692918</td>\n",
       "      <td>-0.691102</td>\n",
       "      <td>-0.922616</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-5.018164</td>\n",
       "      <td>-10.044319</td>\n",
       "      <td>-15.433009</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-2.801296</td>\n",
       "      <td>-3.064457</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>-2.554731</td>\n",
       "      <td>-2.747739</td>\n",
       "      <td>-3.401999</td>\n",
       "      <td>-5.076119</td>\n",
       "      <td>-6.402135</td>\n",
       "      <td>-2.536176</td>\n",
       "      <td>-2.681205</td>\n",
       "      <td>-4.917346</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.106630</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>-0.959475</td>\n",
       "      <td>-0.224630</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>-0.812985</td>\n",
       "      <td>-1.062730</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>-1.339182</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-1.045151</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>-0.611609</td>\n",
       "      <td>-0.729697</td>\n",
       "      <td>-0.824390</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.264350</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.281450</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.301710</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.833173</td>\n",
       "      <td>-0.700755</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>-1.073793</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>-0.529131</td>\n",
       "      <td>-0.633072</td>\n",
       "      <td>-0.779452</td>\n",
       "      <td>-0.027522</td>\n",
       "      <td>-0.666317</td>\n",
       "      <td>-0.664913</td>\n",
       "      <td>-0.734102</td>\n",
       "      <td>-0.941949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.266555</td>\n",
       "      <td>-0.384538</td>\n",
       "      <td>-0.691102</td>\n",
       "      <td>-0.922616</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-0.650591</td>\n",
       "      <td>-0.606906</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.584871</td>\n",
       "      <td>-0.650459</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>-0.791928</td>\n",
       "      <td>-0.563559</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>-0.596008</td>\n",
       "      <td>-0.591330</td>\n",
       "      <td>-0.724419</td>\n",
       "      <td>-0.746978</td>\n",
       "      <td>-0.684580</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.106630</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>-0.959475</td>\n",
       "      <td>-0.224630</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>0.034490</td>\n",
       "      <td>0.089819</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>0.050164</td>\n",
       "      <td>-0.002929</td>\n",
       "      <td>-0.824390</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.264350</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.281450</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.301710</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>0.136975</td>\n",
       "      <td>-0.526853</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>0.931279</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>-0.031580</td>\n",
       "      <td>0.024167</td>\n",
       "      <td>0.133992</td>\n",
       "      <td>-0.032735</td>\n",
       "      <td>0.166549</td>\n",
       "      <td>-0.291123</td>\n",
       "      <td>0.168943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789543</td>\n",
       "      <td>-0.384538</td>\n",
       "      <td>-0.691102</td>\n",
       "      <td>-0.922616</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-0.098831</td>\n",
       "      <td>-0.173314</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>0.058734</td>\n",
       "      <td>0.040694</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>-0.013430</td>\n",
       "      <td>0.057560</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.100070</td>\n",
       "      <td>-0.093259</td>\n",
       "      <td>-0.053080</td>\n",
       "      <td>0.046962</td>\n",
       "      <td>-0.028360</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.555661</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.106630</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>1.042237</td>\n",
       "      <td>-0.224630</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>0.730476</td>\n",
       "      <td>0.898257</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.087080</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>0.521664</td>\n",
       "      <td>0.738003</td>\n",
       "      <td>1.213018</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.264350</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.281450</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.301710</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>0.736130</td>\n",
       "      <td>0.932772</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>0.931279</td>\n",
       "      <td>1.189704</td>\n",
       "      <td>0.560193</td>\n",
       "      <td>0.684167</td>\n",
       "      <td>0.763417</td>\n",
       "      <td>0.381360</td>\n",
       "      <td>0.652617</td>\n",
       "      <td>0.855725</td>\n",
       "      <td>0.810991</td>\n",
       "      <td>0.860520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789543</td>\n",
       "      <td>1.259207</td>\n",
       "      <td>1.446964</td>\n",
       "      <td>1.083874</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.089220</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>0.487269</td>\n",
       "      <td>0.389865</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>0.604263</td>\n",
       "      <td>0.628620</td>\n",
       "      <td>1.087760</td>\n",
       "      <td>0.957662</td>\n",
       "      <td>0.632994</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.563273</td>\n",
       "      <td>0.764848</td>\n",
       "      <td>0.684663</td>\n",
       "      <td>0.644820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.555661</td>\n",
       "      <td>15.099669</td>\n",
       "      <td>3.432588</td>\n",
       "      <td>9.378188</td>\n",
       "      <td>2.444458</td>\n",
       "      <td>6.198783</td>\n",
       "      <td>7.667754</td>\n",
       "      <td>42.341863</td>\n",
       "      <td>5.037847</td>\n",
       "      <td>1.042237</td>\n",
       "      <td>4.451762</td>\n",
       "      <td>73.351892</td>\n",
       "      <td>2.709873</td>\n",
       "      <td>1.804154</td>\n",
       "      <td>2.717565</td>\n",
       "      <td>2.508387</td>\n",
       "      <td>5.329832</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>2.439845</td>\n",
       "      <td>5.553665</td>\n",
       "      <td>25.141950</td>\n",
       "      <td>16.582351</td>\n",
       "      <td>18.312393</td>\n",
       "      <td>25.141950</td>\n",
       "      <td>34.567165</td>\n",
       "      <td>31.264269</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>1.752349</td>\n",
       "      <td>6.722584</td>\n",
       "      <td>2.706299</td>\n",
       "      <td>8.299977</td>\n",
       "      <td>19.940653</td>\n",
       "      <td>4.695435</td>\n",
       "      <td>9.188070</td>\n",
       "      <td>11.483751</td>\n",
       "      <td>22.095866</td>\n",
       "      <td>11.555843</td>\n",
       "      <td>28.756270</td>\n",
       "      <td>51.862800</td>\n",
       "      <td>2.011453</td>\n",
       "      <td>2.059750</td>\n",
       "      <td>4.126557</td>\n",
       "      <td>2.244959</td>\n",
       "      <td>1.791432</td>\n",
       "      <td>3.401999</td>\n",
       "      <td>3.565225</td>\n",
       "      <td>3.960303</td>\n",
       "      <td>2.736449</td>\n",
       "      <td>1.213018</td>\n",
       "      <td>2.144058</td>\n",
       "      <td>2.532298</td>\n",
       "      <td>3.570133</td>\n",
       "      <td>7.342565</td>\n",
       "      <td>2.776316</td>\n",
       "      <td>3.782869</td>\n",
       "      <td>20.724864</td>\n",
       "      <td>10.889853</td>\n",
       "      <td>3.393393</td>\n",
       "      <td>3.656477</td>\n",
       "      <td>3.553034</td>\n",
       "      <td>3.290513</td>\n",
       "      <td>3.500704</td>\n",
       "      <td>6.009475</td>\n",
       "      <td>3.751564</td>\n",
       "      <td>3.410664</td>\n",
       "      <td>3.363725</td>\n",
       "      <td>3.282638</td>\n",
       "      <td>3.314442</td>\n",
       "      <td>3.338840</td>\n",
       "      <td>3.276765</td>\n",
       "      <td>5.936249</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>2.173707</td>\n",
       "      <td>11.208190</td>\n",
       "      <td>6.052509</td>\n",
       "      <td>12.185488</td>\n",
       "      <td>22.095866</td>\n",
       "      <td>8.140562</td>\n",
       "      <td>23.176497</td>\n",
       "      <td>36.665720</td>\n",
       "      <td>42.341863</td>\n",
       "      <td>2.768936</td>\n",
       "      <td>1.920001</td>\n",
       "      <td>4.338359</td>\n",
       "      <td>0.931279</td>\n",
       "      <td>1.189704</td>\n",
       "      <td>10.263668</td>\n",
       "      <td>3.452155</td>\n",
       "      <td>2.781001</td>\n",
       "      <td>40.660902</td>\n",
       "      <td>3.365495</td>\n",
       "      <td>1.833148</td>\n",
       "      <td>2.722573</td>\n",
       "      <td>1.849267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789543</td>\n",
       "      <td>1.259207</td>\n",
       "      <td>1.446964</td>\n",
       "      <td>1.083874</td>\n",
       "      <td>4.256434</td>\n",
       "      <td>3.380593</td>\n",
       "      <td>8.974290</td>\n",
       "      <td>7.141090</td>\n",
       "      <td>10.596921</td>\n",
       "      <td>25.141950</td>\n",
       "      <td>11.208190</td>\n",
       "      <td>20.724864</td>\n",
       "      <td>29.931867</td>\n",
       "      <td>7.778358</td>\n",
       "      <td>13.585616</td>\n",
       "      <td>20.724864</td>\n",
       "      <td>15.789310</td>\n",
       "      <td>27.708946</td>\n",
       "      <td>32.791767</td>\n",
       "      <td>39.199125</td>\n",
       "      <td>103.740060</td>\n",
       "      <td>2.428323</td>\n",
       "      <td>16.723373</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>27.708946</td>\n",
       "      <td>22.616892</td>\n",
       "      <td>42.341863</td>\n",
       "      <td>51.862800</td>\n",
       "      <td>3.858187</td>\n",
       "      <td>6.770266</td>\n",
       "      <td>1.848541</td>\n",
       "      <td>1.900353</td>\n",
       "      <td>7.822027</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>9.966169</td>\n",
       "      <td>3.183437</td>\n",
       "      <td>2.420120</td>\n",
       "      <td>3.100770</td>\n",
       "      <td>3.038261</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  B            BO            CP             E             H  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.005468      0.007569      0.004318      0.001462      0.003624   \n",
       "std        0.997518      1.055366      1.006782      1.006787      1.003707   \n",
       "min       -0.642813     -0.066227     -0.291325     -0.106630     -0.409089   \n",
       "25%       -0.642813     -0.066227     -0.291325     -0.106630     -0.409089   \n",
       "50%       -0.642813     -0.066227     -0.291325     -0.106630     -0.409089   \n",
       "75%        1.555661     -0.066227     -0.291325     -0.106630     -0.409089   \n",
       "max        1.555661     15.099669      3.432588      9.378188      2.444458   \n",
       "\n",
       "                  P            PC            SB            SR            TT  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.006992      0.004887      0.000795      0.001985     -0.005974   \n",
       "std        0.978672      1.018272      1.016706      1.004821      0.999767   \n",
       "min       -0.161322     -0.130416     -0.023617     -0.198497     -0.959475   \n",
       "25%       -0.161322     -0.130416     -0.023617     -0.198497     -0.959475   \n",
       "50%       -0.161322     -0.130416     -0.023617     -0.198497     -0.959475   \n",
       "75%       -0.161322     -0.130416     -0.023617     -0.198497      1.042237   \n",
       "max        6.198783      7.667754     42.341863      5.037847      1.042237   \n",
       "\n",
       "                  V            VO            XB        act_wt        age_re  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.001874     -0.004239      0.011036     -0.007166      0.001429   \n",
       "std        0.996062      0.830169      1.012807      0.994102      1.003374   \n",
       "min       -0.224630     -0.013633     -0.369021     -3.050586     -2.925155   \n",
       "25%       -0.224630     -0.013633     -0.369021     -0.812985     -1.062730   \n",
       "50%       -0.224630     -0.013633     -0.369021      0.034490      0.089819   \n",
       "75%       -0.224630     -0.013633     -0.369021      0.730476      0.898257   \n",
       "max        4.451762     73.351892      2.709873      1.804154      2.717565   \n",
       "\n",
       "       colour_Brown   colour_Grey    colour_Bay  colour_Chestnut  \\\n",
       "count  15619.000000  15619.000000  15619.000000     15619.000000   \n",
       "mean      -0.007433      0.010552     -0.003288         0.006857   \n",
       "std        0.992132      1.026750      1.001000         1.006944   \n",
       "min       -0.398663     -0.187623     -1.339182        -0.409862   \n",
       "25%       -0.398663     -0.187623     -1.339182        -0.409862   \n",
       "50%       -0.398663     -0.187623      0.746725        -0.409862   \n",
       "75%       -0.398663     -0.187623      0.746725        -0.409862   \n",
       "max        2.508387      5.329832      0.746725         2.439845   \n",
       "\n",
       "       colour_BayBrown  colour_Dark BayBrown  colour_BrownGrey  \\\n",
       "count     15619.000000          15619.000000      15619.000000   \n",
       "mean          0.003121             -0.002692         -0.001700   \n",
       "std           1.008379              0.965643          0.985883   \n",
       "min          -0.180061             -0.039774         -0.060305   \n",
       "25%          -0.180061             -0.039774         -0.060305   \n",
       "50%          -0.180061             -0.039774         -0.060305   \n",
       "75%          -0.180061             -0.039774         -0.060305   \n",
       "max           5.553665             25.141950         16.582351   \n",
       "\n",
       "       colour_Dark Bay   colour_Roan  colour_Black  colour_BayGrey  \\\n",
       "count     15619.000000  15619.000000  15619.000000    15619.000000   \n",
       "mean         -0.001691     -0.009141      0.008726       -0.009944   \n",
       "std           0.984476      0.877779      1.140781        0.830277   \n",
       "min          -0.054608     -0.039774     -0.028929       -0.031985   \n",
       "25%          -0.054608     -0.039774     -0.028929       -0.031985   \n",
       "50%          -0.054608     -0.039774     -0.028929       -0.031985   \n",
       "75%          -0.054608     -0.039774     -0.028929       -0.031985   \n",
       "max          18.312393     25.141950     34.567165       31.264269   \n",
       "\n",
       "       country of origin_AUS  country of origin_NZ  country of origin_FR  \\\n",
       "count           15619.000000          15619.000000          15619.000000   \n",
       "mean               -0.004121             -0.003110              0.006545   \n",
       "std                 1.000206              0.998188              1.021297   \n",
       "min                -1.045151             -0.570663             -0.148752   \n",
       "25%                -1.045151             -0.570663             -0.148752   \n",
       "50%                 0.956800             -0.570663             -0.148752   \n",
       "75%                 0.956800             -0.570663             -0.148752   \n",
       "max                 0.956800              1.752349              6.722584   \n",
       "\n",
       "       country of origin_IRE  country of origin_USA  country of origin_CHI  \\\n",
       "count           15619.000000           15619.000000           15619.000000   \n",
       "mean               -0.003224               0.009984              -0.002792   \n",
       "std                 0.996253               1.040016               0.971859   \n",
       "min                -0.369508              -0.120482              -0.050149   \n",
       "25%                -0.369508              -0.120482              -0.050149   \n",
       "50%                -0.369508              -0.120482              -0.050149   \n",
       "75%                -0.369508              -0.120482              -0.050149   \n",
       "max                 2.706299               8.299977              19.940653   \n",
       "\n",
       "       country of origin_GB  country of origin_SAF  country of origin_GER  \\\n",
       "count          15619.000000           15619.000000           15619.000000   \n",
       "mean               0.009208               0.000090               0.001078   \n",
       "std                1.020420               1.000442               1.006154   \n",
       "min               -0.212973              -0.108837              -0.087080   \n",
       "25%               -0.212973              -0.108837              -0.087080   \n",
       "50%               -0.212973              -0.108837              -0.087080   \n",
       "75%               -0.212973              -0.108837              -0.087080   \n",
       "max                4.695435               9.188070              11.483751   \n",
       "\n",
       "       country of origin_JPN  country of origin_BRZ  country of origin_ARG  \\\n",
       "count           15619.000000           15619.000000           15619.000000   \n",
       "mean               -0.005565               0.008129               0.003935   \n",
       "std                 0.936648               1.045582               1.055024   \n",
       "min                -0.045257              -0.086536              -0.034775   \n",
       "25%                -0.045257              -0.086536              -0.034775   \n",
       "50%                -0.045257              -0.086536              -0.034775   \n",
       "75%                -0.045257              -0.086536              -0.034775   \n",
       "max                22.095866              11.555843              28.756270   \n",
       "\n",
       "       country of origin_ITY    course_C+3      course_C    course_A+3  \\\n",
       "count           15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean               -0.005995     -0.002467      0.001587      0.003818   \n",
       "std                 0.830193      0.998160      1.001279      1.007412   \n",
       "min                -0.019282     -0.497153     -0.485496     -0.242333   \n",
       "25%                -0.019282     -0.497153     -0.485496     -0.242333   \n",
       "50%                -0.019282     -0.497153     -0.485496     -0.242333   \n",
       "75%                -0.019282     -0.497153     -0.485496     -0.242333   \n",
       "max                51.862800      2.011453      2.059750      4.126557   \n",
       "\n",
       "           course_B      course_A    course_awt    course_B+2    dam's sire  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean       0.001723     -0.003559      0.005157     -0.003982     -0.040260   \n",
       "std        1.001580      0.997829      1.008002      0.993462      1.064525   \n",
       "min       -0.445442     -0.558213     -0.293945     -0.280487     -2.633267   \n",
       "25%       -0.445442     -0.558213     -0.293945     -0.280487     -0.611609   \n",
       "50%       -0.445442     -0.558213     -0.293945     -0.280487      0.050164   \n",
       "75%       -0.445442     -0.558213     -0.293945     -0.280487      0.521664   \n",
       "max        2.244959      1.791432      3.401999      3.565225      3.960303   \n",
       "\n",
       "       declarhorse_wt   dist_1200.0   dist_1400.0   dist_1650.0   dist_1600.0  \\\n",
       "count    15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean        -0.003702     -0.004679     -0.000771      0.002416      0.007575   \n",
       "std          0.997285      0.999111      0.999385      1.002608      1.012389   \n",
       "min         -3.389061     -0.824390     -0.466405     -0.394898     -0.280102   \n",
       "25%         -0.729697     -0.824390     -0.466405     -0.394898     -0.280102   \n",
       "50%         -0.002929     -0.824390     -0.466405     -0.394898     -0.280102   \n",
       "75%          0.738003      1.213018     -0.466405     -0.394898     -0.280102   \n",
       "max          2.736449      1.213018      2.144058      2.532298      3.570133   \n",
       "\n",
       "        dist_2000.0   dist_1000.0   dist_1800.0   dist_2400.0   dist_2200.0  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean       0.006497      0.000471     -0.006783     -0.001702      0.005902   \n",
       "std        1.023156      1.000601      0.988004      0.982281      1.031389   \n",
       "min       -0.136192     -0.360190     -0.264350     -0.048251     -0.091829   \n",
       "25%       -0.136192     -0.360190     -0.264350     -0.048251     -0.091829   \n",
       "50%       -0.136192     -0.360190     -0.264350     -0.048251     -0.091829   \n",
       "75%       -0.136192     -0.360190     -0.264350     -0.048251     -0.091829   \n",
       "max        7.342565      2.776316      3.782869     20.724864     10.889853   \n",
       "\n",
       "           draw_3.0     draw_11.0      draw_9.0      draw_2.0     draw_10.0  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean       0.010387      0.000269      0.002841      0.002170     -0.003237   \n",
       "std        1.015945      1.000487      1.004664      1.003265      0.994809   \n",
       "min       -0.294690     -0.273487     -0.281450     -0.303904     -0.285657   \n",
       "25%       -0.294690     -0.273487     -0.281450     -0.303904     -0.285657   \n",
       "50%       -0.294690     -0.273487     -0.281450     -0.303904     -0.285657   \n",
       "75%       -0.294690     -0.273487     -0.281450     -0.303904     -0.285657   \n",
       "max        3.393393      3.656477      3.553034      3.290513      3.500704   \n",
       "\n",
       "          draw_14.0     draw_12.0      draw_1.0      draw_7.0      draw_4.0  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.006659     -0.003123      0.005359     -0.004061      0.001292   \n",
       "std        0.980361      0.994570      1.008337      0.993777      1.001953   \n",
       "min       -0.166404     -0.266556     -0.293198     -0.297289     -0.304633   \n",
       "25%       -0.166404     -0.266556     -0.293198     -0.297289     -0.304633   \n",
       "50%       -0.166404     -0.266556     -0.293198     -0.297289     -0.304633   \n",
       "75%       -0.166404     -0.266556     -0.293198     -0.297289     -0.304633   \n",
       "max        6.009475      3.751564      3.410664      3.363725      3.282638   \n",
       "\n",
       "           draw_8.0      draw_6.0      draw_5.0     draw_13.0       going_G  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.001888     -0.004599     -0.005671      0.007036     -0.011833   \n",
       "std        0.997182      0.993008      0.991554      1.020097      1.007296   \n",
       "min       -0.301710     -0.299505     -0.305179     -0.168457     -1.799742   \n",
       "25%       -0.301710     -0.299505     -0.305179     -0.168457      0.555635   \n",
       "50%       -0.301710     -0.299505     -0.305179     -0.168457      0.555635   \n",
       "75%       -0.301710     -0.299505     -0.305179     -0.168457      0.555635   \n",
       "max        3.314442      3.338840      3.276765      5.936249      0.555635   \n",
       "\n",
       "           going_GF      going_WS      going_GY       going_S      going_FT  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean       0.001482      0.014213      0.013123     -0.003522     -0.006983   \n",
       "std        1.001300      1.076061      1.037860      0.978477      0.919799   \n",
       "min       -0.460044     -0.089220     -0.165221     -0.082065     -0.045257   \n",
       "25%       -0.460044     -0.089220     -0.165221     -0.082065     -0.045257   \n",
       "50%       -0.460044     -0.089220     -0.165221     -0.082065     -0.045257   \n",
       "75%       -0.460044     -0.089220     -0.165221     -0.082065     -0.045257   \n",
       "max        2.173707     11.208190      6.052509     12.185488     22.095866   \n",
       "\n",
       "            going_Y       going_H      going_YS      going_WF      horse_id  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean       0.012069     -0.005981      0.019712     -0.001918     -0.014963   \n",
       "std        1.047230      0.928250      1.312222      0.958591      1.023765   \n",
       "min       -0.122842     -0.043147     -0.027273     -0.023617     -1.991612   \n",
       "25%       -0.122842     -0.043147     -0.027273     -0.023617     -0.833173   \n",
       "50%       -0.122842     -0.043147     -0.027273     -0.023617      0.136975   \n",
       "75%       -0.122842     -0.043147     -0.027273     -0.023617      0.736130   \n",
       "max        8.140562     23.176497     36.665720     42.341863      2.768936   \n",
       "\n",
       "            hv_rain  import type_ISG  import type_PPG  import type_PP  \\\n",
       "count  15619.000000     15619.000000     15619.000000    15619.000000   \n",
       "mean       0.007167         0.010242        -0.006236        0.001763   \n",
       "std        1.004300         1.020800         1.000457        1.000338   \n",
       "min       -0.700755        -0.230502        -1.073793       -0.840545   \n",
       "25%       -0.700755        -0.230502        -1.073793       -0.840545   \n",
       "50%       -0.526853        -0.230502         0.931279       -0.840545   \n",
       "75%        0.932772        -0.230502         0.931279        1.189704   \n",
       "max        1.920001         4.338359         0.931279        1.189704   \n",
       "\n",
       "             jockey   last rating   p_odds_allo  pace_sp_shifted  \\\n",
       "count  15619.000000  15619.000000  15619.000000     15619.000000   \n",
       "mean      -0.039693      0.008970     -0.010410        -0.003924   \n",
       "std        1.062729      1.021582      1.005191         0.977427   \n",
       "min       -3.318977     -4.590758     -2.554729        -3.423721   \n",
       "25%       -0.529131     -0.633072     -0.779452        -0.027522   \n",
       "50%        0.075371     -0.031580      0.024167         0.133992   \n",
       "75%        0.560193      0.684167      0.763417         0.381360   \n",
       "max       10.263668      3.452155      2.781001        40.660902   \n",
       "\n",
       "              place  placing_shifted         prize          race  \\\n",
       "count  15619.000000     15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.008286        -0.000816      0.008819     -0.000169   \n",
       "std        1.009228         1.004798      1.014766      1.001574   \n",
       "min       -4.059768        -1.863847     -2.528376     -1.767609   \n",
       "25%       -0.666317        -0.664913     -0.734102     -0.941949   \n",
       "50%       -0.032735         0.166549     -0.291123      0.168943   \n",
       "75%        0.652617         0.855725      0.810991      0.860520   \n",
       "max        3.365495         1.833148      2.722573      1.849267   \n",
       "\n",
       "       raceclass_-1.0            rc     rc_counts  rtg_race_80-60  \\\n",
       "count         15619.0  15619.000000  15619.000000    15619.000000   \n",
       "mean              0.0      0.007202     -0.008974       -0.009122   \n",
       "std               0.0      0.998287      1.014762        0.996537   \n",
       "min               0.0     -1.266555     -2.692918       -0.691102   \n",
       "25%               0.0     -1.266555     -0.384538       -0.691102   \n",
       "50%               0.0      0.789543     -0.384538       -0.691102   \n",
       "75%               0.0      0.789543      1.259207        1.446964   \n",
       "max               0.0      0.789543      1.259207        1.446964   \n",
       "\n",
       "       rtg_race_60-40  rtg_race_100-80  rtg_race_40-0  rtg_race_105-80  \\\n",
       "count    15619.000000     15619.000000   15619.000000     15619.000000   \n",
       "mean        -0.011673         0.003160       0.006892         0.014220   \n",
       "std          0.999022         1.006361       1.010584         1.061083   \n",
       "min         -0.922616        -0.234938      -0.295806        -0.111429   \n",
       "25%         -0.922616        -0.234938      -0.295806        -0.111429   \n",
       "50%         -0.922616        -0.234938      -0.295806        -0.111429   \n",
       "75%          1.083874        -0.234938      -0.295806        -0.111429   \n",
       "max          1.083874         4.256434       3.380593         8.974290   \n",
       "\n",
       "       rtg_race_100-75  rtg_race_60-35  rtg_race_115-90  rtg_race_100-70  \\\n",
       "count     15619.000000    15619.000000     15619.000000     15619.000000   \n",
       "mean          0.025456        0.008309         0.005369         0.000470   \n",
       "std           1.085194        1.042719         1.065276         1.002643   \n",
       "min          -0.140035       -0.094367        -0.039774        -0.089220   \n",
       "25%          -0.140035       -0.094367        -0.039774        -0.089220   \n",
       "50%          -0.140035       -0.094367        -0.039774        -0.089220   \n",
       "75%          -0.140035       -0.094367        -0.039774        -0.089220   \n",
       "max           7.141090       10.596921        25.141950        11.208190   \n",
       "\n",
       "       rtg_race_80-55  rtg_race_90-65  rtg_race_85-60  rtg_race_110-85  \\\n",
       "count    15619.000000    15619.000000    15619.000000     15619.000000   \n",
       "mean         0.008938       -0.006550        0.005591         0.003351   \n",
       "std          1.088490        0.896757        1.021179         1.022416   \n",
       "min         -0.048251       -0.033409       -0.128562        -0.073607   \n",
       "25%         -0.048251       -0.033409       -0.128562        -0.073607   \n",
       "50%         -0.048251       -0.033409       -0.128562        -0.073607   \n",
       "75%         -0.048251       -0.033409       -0.128562        -0.073607   \n",
       "max         20.724864       29.931867        7.778358        13.585616   \n",
       "\n",
       "       rtg_race_90-70  rtg_race_95-75  rtg_race_95+  rtg_race_90+  \\\n",
       "count    15619.000000    15619.000000  15619.000000  15619.000000   \n",
       "mean        -0.000372        0.001623     -0.004115      0.001026   \n",
       "std          0.996183        1.012715      0.941366      1.016700   \n",
       "min         -0.048251       -0.063334     -0.036089     -0.030495   \n",
       "25%         -0.048251       -0.063334     -0.036089     -0.030495   \n",
       "50%         -0.048251       -0.063334     -0.036089     -0.030495   \n",
       "75%         -0.048251       -0.063334     -0.036089     -0.030495   \n",
       "max         20.724864       15.789310     27.708946     32.791767   \n",
       "\n",
       "       rtg_race_105-75  rtg_race_110-80  same_sire_score  season stakes*  \\\n",
       "count     15619.000000     15619.000000     15619.000000    15619.000000   \n",
       "mean         -0.000397         0.030216        -0.003301        0.018706   \n",
       "std           0.992218         2.033136         1.002371        1.068798   \n",
       "min          -0.025511        -0.009639        -5.018164      -10.044319   \n",
       "25%          -0.025511        -0.009639        -0.650591       -0.606906   \n",
       "50%          -0.025511        -0.009639        -0.098831       -0.173314   \n",
       "75%          -0.025511        -0.009639         0.487269        0.389865   \n",
       "max          39.199125       103.740060         2.428323       16.723373   \n",
       "\n",
       "        sex_Gelding      sex_Colt     sex_Horse      sex_Mare     sex_Filly  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.007637      0.004767      0.008017     -0.001918     -0.002673   \n",
       "std        1.057065      1.063938      1.086722      0.958591      0.928154   \n",
       "min      -15.433009     -0.036089     -0.044215     -0.023617     -0.019282   \n",
       "25%        0.064796     -0.036089     -0.044215     -0.023617     -0.019282   \n",
       "50%        0.064796     -0.036089     -0.044215     -0.023617     -0.019282   \n",
       "75%        0.064796     -0.036089     -0.044215     -0.023617     -0.019282   \n",
       "max        0.064796     27.708946     22.616892     42.341863     51.862800   \n",
       "\n",
       "               sire    sire_score       st_rain       st_temp  total stakes*  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000   15619.000000   \n",
       "mean      -0.021003      0.007456      0.003826     -0.008984       0.013711   \n",
       "std        1.064995      1.019167      1.002844      1.008005       1.036953   \n",
       "min       -2.801296     -3.064457     -0.635676     -2.554731      -2.747739   \n",
       "25%       -0.584871     -0.650459     -0.635676     -0.791928      -0.563559   \n",
       "50%        0.058734      0.040694     -0.635676     -0.013430       0.057560   \n",
       "75%        0.604263      0.628620      1.087760      0.957662       0.632994   \n",
       "max        3.858187      6.770266      1.848541      1.900353       7.822027   \n",
       "\n",
       "              track       trainer      trot_sum        w_odds   w_odds_allo  \\\n",
       "count  15619.000000  15619.000000  15619.000000  15619.000000  15619.000000   \n",
       "mean      -0.005157     -0.066382      0.000585      0.008251     -0.009746   \n",
       "std        1.008002      1.083616      1.000158      1.005601      1.005343   \n",
       "min       -3.401999     -5.076119     -6.402135     -2.536176     -2.681205   \n",
       "25%        0.293945     -0.596008     -0.591330     -0.724419     -0.746978   \n",
       "50%        0.293945      0.100070     -0.093259     -0.053080      0.046962   \n",
       "75%        0.293945      0.504300      0.563273      0.764848      0.684663   \n",
       "max        0.293945      9.966169      3.183437      2.420120      3.100770   \n",
       "\n",
       "                win          top3  \n",
       "count  15619.000000  15619.000000  \n",
       "mean      -0.007215      0.216915  \n",
       "std        1.007724      0.412158  \n",
       "min       -4.917346      0.000000  \n",
       "25%       -0.684580      0.000000  \n",
       "50%       -0.028360      0.000000  \n",
       "75%        0.644820      0.000000  \n",
       "max        3.038261      1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "get_config('dataset_transformed').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0451cbe-4204-4f72-b2e9-e0213056ed0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from category_encoders import CatBoostEncoder\n",
    "# encoder = CatBoostEncoder()\n",
    "\n",
    "# t1_model = setup(data = top1_df, target = 'top1', experiment_name = 'Top1 predict', train_size=0.7,\n",
    "#                  fix_imbalance = True,fix_imbalance_method = 'SMOTE',\n",
    "#                  remove_outliers = True,\n",
    "#                  # imputation_type = 'iterative',\n",
    "#                 # numeric_imputation = 'mode',\n",
    "#                 #  categorical_imputation = 'mode',\n",
    "#                  # numeric_iterative_imputer = 'rf',\n",
    "#                  # categorical_iterative_imputer = 'rf',\n",
    "#                  #use_gpu = True,\n",
    "#                  # fold = 20,\n",
    "#                  remove_multicollinearity = True,\n",
    "#                  multicollinearity_threshold = 0.9,\n",
    "#                  # feature_selection = True, \n",
    "#                  # feature_selection_estimator = 'rf', feature_selection_method = 'classic',\n",
    "#                  # n_features_to_select = 0.3,\n",
    "#                  # pca = True,\n",
    "#                  max_encoding_ohe = 15,\n",
    "#                  categorical_features = categorical_features,\n",
    "#                  encoding_method = encoder,\n",
    "#                  ignore_features = ignore_features,\n",
    "#                  # numeric_features = numeric_features,\n",
    "#                  transformation = True,\n",
    "#                  data_split_stratify = True, \n",
    "#                  normalize = True,\n",
    "#                  session_id=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2897d-3fd5-4b45-a604-abbf1670aff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3235d623-e202-4edd-9da4-eec1629d5eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>BO</th>\n",
       "      <th>CP</th>\n",
       "      <th>E</th>\n",
       "      <th>H</th>\n",
       "      <th>P</th>\n",
       "      <th>PC</th>\n",
       "      <th>SB</th>\n",
       "      <th>SR</th>\n",
       "      <th>TT</th>\n",
       "      <th>V</th>\n",
       "      <th>VO</th>\n",
       "      <th>XB</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>age_re</th>\n",
       "      <th>colour_Brown</th>\n",
       "      <th>colour_Grey</th>\n",
       "      <th>colour_Bay</th>\n",
       "      <th>colour_Chestnut</th>\n",
       "      <th>colour_BayBrown</th>\n",
       "      <th>colour_Dark BayBrown</th>\n",
       "      <th>colour_BrownGrey</th>\n",
       "      <th>colour_Dark Bay</th>\n",
       "      <th>colour_Roan</th>\n",
       "      <th>colour_Black</th>\n",
       "      <th>colour_BayGrey</th>\n",
       "      <th>country of origin_AUS</th>\n",
       "      <th>country of origin_NZ</th>\n",
       "      <th>country of origin_FR</th>\n",
       "      <th>country of origin_IRE</th>\n",
       "      <th>country of origin_USA</th>\n",
       "      <th>country of origin_CHI</th>\n",
       "      <th>country of origin_GB</th>\n",
       "      <th>country of origin_SAF</th>\n",
       "      <th>country of origin_GER</th>\n",
       "      <th>country of origin_JPN</th>\n",
       "      <th>country of origin_BRZ</th>\n",
       "      <th>country of origin_ARG</th>\n",
       "      <th>country of origin_ITY</th>\n",
       "      <th>course_C+3</th>\n",
       "      <th>course_C</th>\n",
       "      <th>course_A+3</th>\n",
       "      <th>course_B</th>\n",
       "      <th>course_A</th>\n",
       "      <th>course_awt</th>\n",
       "      <th>course_B+2</th>\n",
       "      <th>dam's sire</th>\n",
       "      <th>declarhorse_wt</th>\n",
       "      <th>dist_1200.0</th>\n",
       "      <th>dist_1400.0</th>\n",
       "      <th>dist_1650.0</th>\n",
       "      <th>dist_1600.0</th>\n",
       "      <th>dist_2000.0</th>\n",
       "      <th>dist_1000.0</th>\n",
       "      <th>dist_1800.0</th>\n",
       "      <th>dist_2400.0</th>\n",
       "      <th>dist_2200.0</th>\n",
       "      <th>draw_3.0</th>\n",
       "      <th>draw_11.0</th>\n",
       "      <th>draw_9.0</th>\n",
       "      <th>draw_2.0</th>\n",
       "      <th>draw_10.0</th>\n",
       "      <th>draw_14.0</th>\n",
       "      <th>draw_12.0</th>\n",
       "      <th>draw_1.0</th>\n",
       "      <th>draw_7.0</th>\n",
       "      <th>draw_4.0</th>\n",
       "      <th>draw_8.0</th>\n",
       "      <th>draw_6.0</th>\n",
       "      <th>draw_5.0</th>\n",
       "      <th>draw_13.0</th>\n",
       "      <th>going_G</th>\n",
       "      <th>going_GF</th>\n",
       "      <th>going_WS</th>\n",
       "      <th>going_GY</th>\n",
       "      <th>going_S</th>\n",
       "      <th>going_FT</th>\n",
       "      <th>going_Y</th>\n",
       "      <th>going_H</th>\n",
       "      <th>going_YS</th>\n",
       "      <th>going_WF</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>hv_rain</th>\n",
       "      <th>import type_ISG</th>\n",
       "      <th>import type_PPG</th>\n",
       "      <th>import type_PP</th>\n",
       "      <th>jockey</th>\n",
       "      <th>last rating</th>\n",
       "      <th>p_odds_allo</th>\n",
       "      <th>pace_sp_shifted</th>\n",
       "      <th>place</th>\n",
       "      <th>placing_shifted</th>\n",
       "      <th>prize</th>\n",
       "      <th>race</th>\n",
       "      <th>raceclass_-1.0</th>\n",
       "      <th>rc</th>\n",
       "      <th>rc_counts</th>\n",
       "      <th>rtg_race_80-60</th>\n",
       "      <th>rtg_race_60-40</th>\n",
       "      <th>rtg_race_100-80</th>\n",
       "      <th>rtg_race_40-0</th>\n",
       "      <th>rtg_race_105-80</th>\n",
       "      <th>rtg_race_100-75</th>\n",
       "      <th>rtg_race_60-35</th>\n",
       "      <th>rtg_race_115-90</th>\n",
       "      <th>rtg_race_100-70</th>\n",
       "      <th>rtg_race_80-55</th>\n",
       "      <th>rtg_race_90-65</th>\n",
       "      <th>rtg_race_85-60</th>\n",
       "      <th>rtg_race_110-85</th>\n",
       "      <th>rtg_race_90-70</th>\n",
       "      <th>rtg_race_95-75</th>\n",
       "      <th>rtg_race_95+</th>\n",
       "      <th>rtg_race_90+</th>\n",
       "      <th>rtg_race_105-75</th>\n",
       "      <th>rtg_race_110-80</th>\n",
       "      <th>same_sire_score</th>\n",
       "      <th>season stakes*</th>\n",
       "      <th>sex_Gelding</th>\n",
       "      <th>sex_Colt</th>\n",
       "      <th>sex_Horse</th>\n",
       "      <th>sex_Mare</th>\n",
       "      <th>sex_Filly</th>\n",
       "      <th>sire</th>\n",
       "      <th>sire_score</th>\n",
       "      <th>st_rain</th>\n",
       "      <th>st_temp</th>\n",
       "      <th>total stakes*</th>\n",
       "      <th>track</th>\n",
       "      <th>trainer</th>\n",
       "      <th>trot_sum</th>\n",
       "      <th>w_odds</th>\n",
       "      <th>w_odds_allo</th>\n",
       "      <th>win</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11511</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.10663</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>1.042237</td>\n",
       "      <td>-0.22463</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>1.262882</td>\n",
       "      <td>0.089819</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>5.329832</td>\n",
       "      <td>-1.339182</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-1.045151</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>6.722584</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.08708</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>2.244959</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>-1.285379</td>\n",
       "      <td>-0.063818</td>\n",
       "      <td>1.213018</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.26435</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.28145</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>3.282638</td>\n",
       "      <td>-0.30171</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-1.846030</td>\n",
       "      <td>-0.526853</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>0.931279</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>-0.351372</td>\n",
       "      <td>1.065962</td>\n",
       "      <td>1.752389</td>\n",
       "      <td>0.260497</td>\n",
       "      <td>1.137540</td>\n",
       "      <td>-0.365255</td>\n",
       "      <td>0.586720</td>\n",
       "      <td>1.195790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.266555</td>\n",
       "      <td>-0.384538</td>\n",
       "      <td>1.446964</td>\n",
       "      <td>-0.922616</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>1.214202</td>\n",
       "      <td>0.994478</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.913652</td>\n",
       "      <td>0.881567</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>0.384668</td>\n",
       "      <td>1.187188</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.212348</td>\n",
       "      <td>-0.432277</td>\n",
       "      <td>-1.295509</td>\n",
       "      <td>1.514223</td>\n",
       "      <td>1.043573</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.10663</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>1.042237</td>\n",
       "      <td>-0.22463</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>-1.472789</td>\n",
       "      <td>0.089819</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-1.045151</td>\n",
       "      <td>1.752349</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.08708</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>3.565225</td>\n",
       "      <td>-0.409379</td>\n",
       "      <td>-0.946160</td>\n",
       "      <td>1.213018</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.26435</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.28145</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.30171</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>5.936249</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.220403</td>\n",
       "      <td>-0.526853</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>0.931279</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>-0.910387</td>\n",
       "      <td>0.155601</td>\n",
       "      <td>-1.400287</td>\n",
       "      <td>-0.139494</td>\n",
       "      <td>1.722082</td>\n",
       "      <td>0.637133</td>\n",
       "      <td>0.810991</td>\n",
       "      <td>1.525163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789543</td>\n",
       "      <td>1.259207</td>\n",
       "      <td>1.446964</td>\n",
       "      <td>-0.922616</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-0.892631</td>\n",
       "      <td>-0.613138</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.648563</td>\n",
       "      <td>-0.575140</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>-1.062087</td>\n",
       "      <td>-0.869251</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>-0.555743</td>\n",
       "      <td>-0.649618</td>\n",
       "      <td>1.645869</td>\n",
       "      <td>-1.368502</td>\n",
       "      <td>1.798651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.10663</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>-0.959475</td>\n",
       "      <td>-0.22463</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>1.622748</td>\n",
       "      <td>1.990787</td>\n",
       "      <td>2.508387</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>-1.339182</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-1.045151</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>2.706299</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.08708</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>2.244959</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>-1.015266</td>\n",
       "      <td>-0.879010</td>\n",
       "      <td>1.213018</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.26435</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>3.393393</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.28145</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.30171</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.475918</td>\n",
       "      <td>1.904904</td>\n",
       "      <td>4.338359</td>\n",
       "      <td>-1.073793</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>0.163293</td>\n",
       "      <td>-0.031580</td>\n",
       "      <td>1.138587</td>\n",
       "      <td>0.041286</td>\n",
       "      <td>-0.573203</td>\n",
       "      <td>-0.089978</td>\n",
       "      <td>-0.585630</td>\n",
       "      <td>-0.941949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.266555</td>\n",
       "      <td>-0.384538</td>\n",
       "      <td>-0.691102</td>\n",
       "      <td>1.083874</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-0.459005</td>\n",
       "      <td>-0.567758</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.863391</td>\n",
       "      <td>0.260768</td>\n",
       "      <td>1.820554</td>\n",
       "      <td>0.822593</td>\n",
       "      <td>0.349859</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>-0.488974</td>\n",
       "      <td>0.543728</td>\n",
       "      <td>-1.268722</td>\n",
       "      <td>0.930715</td>\n",
       "      <td>-1.278335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>-0.642813</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.10663</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>-0.959475</td>\n",
       "      <td>-0.22463</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>2.709873</td>\n",
       "      <td>1.084425</td>\n",
       "      <td>-1.062730</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>5.329832</td>\n",
       "      <td>-1.339182</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-1.045151</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>2.706299</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.08708</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.497153</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>1.791432</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>-2.092949</td>\n",
       "      <td>0.192080</td>\n",
       "      <td>-0.824390</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>7.342565</td>\n",
       "      <td>-0.360190</td>\n",
       "      <td>-0.26435</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.28145</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>3.363725</td>\n",
       "      <td>-0.304633</td>\n",
       "      <td>-0.30171</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>-1.799742</td>\n",
       "      <td>2.173707</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-1.568581</td>\n",
       "      <td>-0.700755</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>-1.073793</td>\n",
       "      <td>1.189704</td>\n",
       "      <td>-1.166546</td>\n",
       "      <td>1.118668</td>\n",
       "      <td>0.488443</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.492732</td>\n",
       "      <td>0.855725</td>\n",
       "      <td>0.377623</td>\n",
       "      <td>1.195790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789543</td>\n",
       "      <td>1.259207</td>\n",
       "      <td>1.446964</td>\n",
       "      <td>-0.922616</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-0.650591</td>\n",
       "      <td>1.563481</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-2.466215</td>\n",
       "      <td>0.931365</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>-0.255616</td>\n",
       "      <td>1.768796</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>-1.643833</td>\n",
       "      <td>0.461458</td>\n",
       "      <td>-0.375581</td>\n",
       "      <td>0.548316</td>\n",
       "      <td>0.812336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>1.555661</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>-0.291325</td>\n",
       "      <td>-0.10663</td>\n",
       "      <td>-0.409089</td>\n",
       "      <td>-0.161322</td>\n",
       "      <td>-0.130416</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.198497</td>\n",
       "      <td>-0.959475</td>\n",
       "      <td>-0.22463</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.369021</td>\n",
       "      <td>-2.274584</td>\n",
       "      <td>-1.062730</td>\n",
       "      <td>-0.398663</td>\n",
       "      <td>-0.187623</td>\n",
       "      <td>-1.339182</td>\n",
       "      <td>-0.409862</td>\n",
       "      <td>-0.180061</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>16.582351</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>-0.570663</td>\n",
       "      <td>-0.148752</td>\n",
       "      <td>-0.369508</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.050149</td>\n",
       "      <td>-0.212973</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.08708</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.086536</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>2.011453</td>\n",
       "      <td>-0.485496</td>\n",
       "      <td>-0.242333</td>\n",
       "      <td>-0.445442</td>\n",
       "      <td>-0.558213</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>-0.280487</td>\n",
       "      <td>-0.859691</td>\n",
       "      <td>0.057540</td>\n",
       "      <td>-0.824390</td>\n",
       "      <td>-0.466405</td>\n",
       "      <td>-0.394898</td>\n",
       "      <td>-0.280102</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>2.776316</td>\n",
       "      <td>-0.26435</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.091829</td>\n",
       "      <td>-0.294690</td>\n",
       "      <td>-0.273487</td>\n",
       "      <td>-0.28145</td>\n",
       "      <td>-0.303904</td>\n",
       "      <td>-0.285657</td>\n",
       "      <td>-0.166404</td>\n",
       "      <td>-0.266556</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>-0.297289</td>\n",
       "      <td>3.282638</td>\n",
       "      <td>-0.30171</td>\n",
       "      <td>-0.299505</td>\n",
       "      <td>-0.305179</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.555635</td>\n",
       "      <td>-0.460044</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.165221</td>\n",
       "      <td>-0.082065</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.122842</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.571436</td>\n",
       "      <td>-0.700755</td>\n",
       "      <td>-0.230502</td>\n",
       "      <td>0.931279</td>\n",
       "      <td>-0.840545</td>\n",
       "      <td>-0.654889</td>\n",
       "      <td>-1.309484</td>\n",
       "      <td>-0.891414</td>\n",
       "      <td>0.296811</td>\n",
       "      <td>-0.607108</td>\n",
       "      <td>1.065292</td>\n",
       "      <td>-0.734102</td>\n",
       "      <td>-0.941949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.266555</td>\n",
       "      <td>-0.384538</td>\n",
       "      <td>-0.691102</td>\n",
       "      <td>1.083874</td>\n",
       "      <td>-0.234938</td>\n",
       "      <td>-0.295806</td>\n",
       "      <td>-0.111429</td>\n",
       "      <td>-0.140035</td>\n",
       "      <td>-0.094367</td>\n",
       "      <td>-0.039774</td>\n",
       "      <td>-0.08922</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.128562</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.048251</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.030495</td>\n",
       "      <td>-0.025511</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>0.918267</td>\n",
       "      <td>-0.850567</td>\n",
       "      <td>0.064796</td>\n",
       "      <td>-0.036089</td>\n",
       "      <td>-0.044215</td>\n",
       "      <td>-0.023617</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.192554</td>\n",
       "      <td>-0.824568</td>\n",
       "      <td>-0.635676</td>\n",
       "      <td>0.027636</td>\n",
       "      <td>-0.549389</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>-1.324460</td>\n",
       "      <td>-3.465528</td>\n",
       "      <td>0.681539</td>\n",
       "      <td>-0.715906</td>\n",
       "      <td>-0.370200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              B        BO        CP        E         H         P        PC  \\\n",
       "11511 -0.642813 -0.066227 -0.291325 -0.10663 -0.409089 -0.161322 -0.130416   \n",
       "7311  -0.642813 -0.066227 -0.291325 -0.10663 -0.409089 -0.161322 -0.130416   \n",
       "4788  -0.642813 -0.066227 -0.291325 -0.10663 -0.409089 -0.161322 -0.130416   \n",
       "16133 -0.642813 -0.066227 -0.291325 -0.10663 -0.409089 -0.161322 -0.130416   \n",
       "13498  1.555661 -0.066227 -0.291325 -0.10663 -0.409089 -0.161322 -0.130416   \n",
       "\n",
       "             SB        SR        TT        V        VO        XB    act_wt  \\\n",
       "11511 -0.023617 -0.198497  1.042237 -0.22463 -0.013633 -0.369021  1.262882   \n",
       "7311  -0.023617 -0.198497  1.042237 -0.22463 -0.013633 -0.369021 -1.472789   \n",
       "4788  -0.023617 -0.198497 -0.959475 -0.22463 -0.013633 -0.369021  1.622748   \n",
       "16133 -0.023617 -0.198497 -0.959475 -0.22463 -0.013633  2.709873  1.084425   \n",
       "13498 -0.023617 -0.198497 -0.959475 -0.22463 -0.013633 -0.369021 -2.274584   \n",
       "\n",
       "         age_re  colour_Brown  colour_Grey  colour_Bay  colour_Chestnut  \\\n",
       "11511  0.089819     -0.398663     5.329832   -1.339182        -0.409862   \n",
       "7311   0.089819     -0.398663    -0.187623    0.746725        -0.409862   \n",
       "4788   1.990787      2.508387    -0.187623   -1.339182        -0.409862   \n",
       "16133 -1.062730     -0.398663     5.329832   -1.339182        -0.409862   \n",
       "13498 -1.062730     -0.398663    -0.187623   -1.339182        -0.409862   \n",
       "\n",
       "       colour_BayBrown  colour_Dark BayBrown  colour_BrownGrey  \\\n",
       "11511        -0.180061             -0.039774         -0.060305   \n",
       "7311         -0.180061             -0.039774         -0.060305   \n",
       "4788         -0.180061             -0.039774         -0.060305   \n",
       "16133        -0.180061             -0.039774         -0.060305   \n",
       "13498        -0.180061             -0.039774         16.582351   \n",
       "\n",
       "       colour_Dark Bay  colour_Roan  colour_Black  colour_BayGrey  \\\n",
       "11511        -0.054608    -0.039774     -0.028929       -0.031985   \n",
       "7311         -0.054608    -0.039774     -0.028929       -0.031985   \n",
       "4788         -0.054608    -0.039774     -0.028929       -0.031985   \n",
       "16133        -0.054608    -0.039774     -0.028929       -0.031985   \n",
       "13498        -0.054608    -0.039774     -0.028929       -0.031985   \n",
       "\n",
       "       country of origin_AUS  country of origin_NZ  country of origin_FR  \\\n",
       "11511              -1.045151             -0.570663              6.722584   \n",
       "7311               -1.045151              1.752349             -0.148752   \n",
       "4788               -1.045151             -0.570663             -0.148752   \n",
       "16133              -1.045151             -0.570663             -0.148752   \n",
       "13498               0.956800             -0.570663             -0.148752   \n",
       "\n",
       "       country of origin_IRE  country of origin_USA  country of origin_CHI  \\\n",
       "11511              -0.369508              -0.120482              -0.050149   \n",
       "7311               -0.369508              -0.120482              -0.050149   \n",
       "4788                2.706299              -0.120482              -0.050149   \n",
       "16133               2.706299              -0.120482              -0.050149   \n",
       "13498              -0.369508              -0.120482              -0.050149   \n",
       "\n",
       "       country of origin_GB  country of origin_SAF  country of origin_GER  \\\n",
       "11511             -0.212973              -0.108837               -0.08708   \n",
       "7311              -0.212973              -0.108837               -0.08708   \n",
       "4788              -0.212973              -0.108837               -0.08708   \n",
       "16133             -0.212973              -0.108837               -0.08708   \n",
       "13498             -0.212973              -0.108837               -0.08708   \n",
       "\n",
       "       country of origin_JPN  country of origin_BRZ  country of origin_ARG  \\\n",
       "11511              -0.045257              -0.086536              -0.034775   \n",
       "7311               -0.045257              -0.086536              -0.034775   \n",
       "4788               -0.045257              -0.086536              -0.034775   \n",
       "16133              -0.045257              -0.086536              -0.034775   \n",
       "13498              -0.045257              -0.086536              -0.034775   \n",
       "\n",
       "       country of origin_ITY  course_C+3  course_C  course_A+3  course_B  \\\n",
       "11511              -0.019282   -0.497153 -0.485496   -0.242333  2.244959   \n",
       "7311               -0.019282   -0.497153 -0.485496   -0.242333 -0.445442   \n",
       "4788               -0.019282   -0.497153 -0.485496   -0.242333  2.244959   \n",
       "16133              -0.019282   -0.497153 -0.485496   -0.242333 -0.445442   \n",
       "13498              -0.019282    2.011453 -0.485496   -0.242333 -0.445442   \n",
       "\n",
       "       course_A  course_awt  course_B+2  dam's sire  declarhorse_wt  \\\n",
       "11511 -0.558213   -0.293945   -0.280487   -1.285379       -0.063818   \n",
       "7311  -0.558213   -0.293945    3.565225   -0.409379       -0.946160   \n",
       "4788  -0.558213   -0.293945   -0.280487   -1.015266       -0.879010   \n",
       "16133  1.791432   -0.293945   -0.280487   -2.092949        0.192080   \n",
       "13498 -0.558213   -0.293945   -0.280487   -0.859691        0.057540   \n",
       "\n",
       "       dist_1200.0  dist_1400.0  dist_1650.0  dist_1600.0  dist_2000.0  \\\n",
       "11511     1.213018    -0.466405    -0.394898    -0.280102    -0.136192   \n",
       "7311      1.213018    -0.466405    -0.394898    -0.280102    -0.136192   \n",
       "4788      1.213018    -0.466405    -0.394898    -0.280102    -0.136192   \n",
       "16133    -0.824390    -0.466405    -0.394898    -0.280102     7.342565   \n",
       "13498    -0.824390    -0.466405    -0.394898    -0.280102    -0.136192   \n",
       "\n",
       "       dist_1000.0  dist_1800.0  dist_2400.0  dist_2200.0  draw_3.0  \\\n",
       "11511    -0.360190     -0.26435    -0.048251    -0.091829 -0.294690   \n",
       "7311     -0.360190     -0.26435    -0.048251    -0.091829 -0.294690   \n",
       "4788     -0.360190     -0.26435    -0.048251    -0.091829  3.393393   \n",
       "16133    -0.360190     -0.26435    -0.048251    -0.091829 -0.294690   \n",
       "13498     2.776316     -0.26435    -0.048251    -0.091829 -0.294690   \n",
       "\n",
       "       draw_11.0  draw_9.0  draw_2.0  draw_10.0  draw_14.0  draw_12.0  \\\n",
       "11511  -0.273487  -0.28145 -0.303904  -0.285657  -0.166404  -0.266556   \n",
       "7311   -0.273487  -0.28145 -0.303904  -0.285657  -0.166404  -0.266556   \n",
       "4788   -0.273487  -0.28145 -0.303904  -0.285657  -0.166404  -0.266556   \n",
       "16133  -0.273487  -0.28145 -0.303904  -0.285657  -0.166404  -0.266556   \n",
       "13498  -0.273487  -0.28145 -0.303904  -0.285657  -0.166404  -0.266556   \n",
       "\n",
       "       draw_1.0  draw_7.0  draw_4.0  draw_8.0  draw_6.0  draw_5.0  draw_13.0  \\\n",
       "11511 -0.293198 -0.297289  3.282638  -0.30171 -0.299505 -0.305179  -0.168457   \n",
       "7311  -0.293198 -0.297289 -0.304633  -0.30171 -0.299505 -0.305179   5.936249   \n",
       "4788  -0.293198 -0.297289 -0.304633  -0.30171 -0.299505 -0.305179  -0.168457   \n",
       "16133 -0.293198  3.363725 -0.304633  -0.30171 -0.299505 -0.305179  -0.168457   \n",
       "13498 -0.293198 -0.297289  3.282638  -0.30171 -0.299505 -0.305179  -0.168457   \n",
       "\n",
       "        going_G  going_GF  going_WS  going_GY   going_S  going_FT   going_Y  \\\n",
       "11511  0.555635 -0.460044  -0.08922 -0.165221 -0.082065 -0.045257 -0.122842   \n",
       "7311   0.555635 -0.460044  -0.08922 -0.165221 -0.082065 -0.045257 -0.122842   \n",
       "4788   0.555635 -0.460044  -0.08922 -0.165221 -0.082065 -0.045257 -0.122842   \n",
       "16133 -1.799742  2.173707  -0.08922 -0.165221 -0.082065 -0.045257 -0.122842   \n",
       "13498  0.555635 -0.460044  -0.08922 -0.165221 -0.082065 -0.045257 -0.122842   \n",
       "\n",
       "        going_H  going_YS  going_WF  horse_id   hv_rain  import type_ISG  \\\n",
       "11511 -0.043147 -0.027273 -0.023617 -1.846030 -0.526853        -0.230502   \n",
       "7311  -0.043147 -0.027273 -0.023617 -0.220403 -0.526853        -0.230502   \n",
       "4788  -0.043147 -0.027273 -0.023617 -0.475918  1.904904         4.338359   \n",
       "16133 -0.043147 -0.027273 -0.023617 -1.568581 -0.700755        -0.230502   \n",
       "13498 -0.043147 -0.027273 -0.023617 -0.571436 -0.700755        -0.230502   \n",
       "\n",
       "       import type_PPG  import type_PP    jockey  last rating  p_odds_allo  \\\n",
       "11511         0.931279       -0.840545 -0.351372     1.065962     1.752389   \n",
       "7311          0.931279       -0.840545 -0.910387     0.155601    -1.400287   \n",
       "4788         -1.073793       -0.840545  0.163293    -0.031580     1.138587   \n",
       "16133        -1.073793        1.189704 -1.166546     1.118668     0.488443   \n",
       "13498         0.931279       -0.840545 -0.654889    -1.309484    -0.891414   \n",
       "\n",
       "       pace_sp_shifted     place  placing_shifted     prize      race  \\\n",
       "11511         0.260497  1.137540        -0.365255  0.586720  1.195790   \n",
       "7311         -0.139494  1.722082         0.637133  0.810991  1.525163   \n",
       "4788          0.041286 -0.573203        -0.089978 -0.585630 -0.941949   \n",
       "16133         0.206082  0.492732         0.855725  0.377623  1.195790   \n",
       "13498         0.296811 -0.607108         1.065292 -0.734102 -0.941949   \n",
       "\n",
       "       raceclass_-1.0        rc  rc_counts  rtg_race_80-60  rtg_race_60-40  \\\n",
       "11511             0.0 -1.266555  -0.384538        1.446964       -0.922616   \n",
       "7311              0.0  0.789543   1.259207        1.446964       -0.922616   \n",
       "4788              0.0 -1.266555  -0.384538       -0.691102        1.083874   \n",
       "16133             0.0  0.789543   1.259207        1.446964       -0.922616   \n",
       "13498             0.0 -1.266555  -0.384538       -0.691102        1.083874   \n",
       "\n",
       "       rtg_race_100-80  rtg_race_40-0  rtg_race_105-80  rtg_race_100-75  \\\n",
       "11511        -0.234938      -0.295806        -0.111429        -0.140035   \n",
       "7311         -0.234938      -0.295806        -0.111429        -0.140035   \n",
       "4788         -0.234938      -0.295806        -0.111429        -0.140035   \n",
       "16133        -0.234938      -0.295806        -0.111429        -0.140035   \n",
       "13498        -0.234938      -0.295806        -0.111429        -0.140035   \n",
       "\n",
       "       rtg_race_60-35  rtg_race_115-90  rtg_race_100-70  rtg_race_80-55  \\\n",
       "11511       -0.094367        -0.039774         -0.08922       -0.048251   \n",
       "7311        -0.094367        -0.039774         -0.08922       -0.048251   \n",
       "4788        -0.094367        -0.039774         -0.08922       -0.048251   \n",
       "16133       -0.094367        -0.039774         -0.08922       -0.048251   \n",
       "13498       -0.094367        -0.039774         -0.08922       -0.048251   \n",
       "\n",
       "       rtg_race_90-65  rtg_race_85-60  rtg_race_110-85  rtg_race_90-70  \\\n",
       "11511       -0.033409       -0.128562        -0.073607       -0.048251   \n",
       "7311        -0.033409       -0.128562        -0.073607       -0.048251   \n",
       "4788        -0.033409       -0.128562        -0.073607       -0.048251   \n",
       "16133       -0.033409       -0.128562        -0.073607       -0.048251   \n",
       "13498       -0.033409       -0.128562        -0.073607       -0.048251   \n",
       "\n",
       "       rtg_race_95-75  rtg_race_95+  rtg_race_90+  rtg_race_105-75  \\\n",
       "11511       -0.063334     -0.036089     -0.030495        -0.025511   \n",
       "7311        -0.063334     -0.036089     -0.030495        -0.025511   \n",
       "4788        -0.063334     -0.036089     -0.030495        -0.025511   \n",
       "16133       -0.063334     -0.036089     -0.030495        -0.025511   \n",
       "13498       -0.063334     -0.036089     -0.030495        -0.025511   \n",
       "\n",
       "       rtg_race_110-80  same_sire_score  season stakes*  sex_Gelding  \\\n",
       "11511        -0.009639         1.214202        0.994478     0.064796   \n",
       "7311         -0.009639        -0.892631       -0.613138     0.064796   \n",
       "4788         -0.009639        -0.459005       -0.567758     0.064796   \n",
       "16133        -0.009639        -0.650591        1.563481     0.064796   \n",
       "13498        -0.009639         0.918267       -0.850567     0.064796   \n",
       "\n",
       "       sex_Colt  sex_Horse  sex_Mare  sex_Filly      sire  sire_score  \\\n",
       "11511 -0.036089  -0.044215 -0.023617  -0.019282 -0.913652    0.881567   \n",
       "7311  -0.036089  -0.044215 -0.023617  -0.019282 -0.648563   -0.575140   \n",
       "4788  -0.036089  -0.044215 -0.023617  -0.019282 -0.863391    0.260768   \n",
       "16133 -0.036089  -0.044215 -0.023617  -0.019282 -2.466215    0.931365   \n",
       "13498 -0.036089  -0.044215 -0.023617  -0.019282 -0.192554   -0.824568   \n",
       "\n",
       "        st_rain   st_temp  total stakes*     track   trainer  trot_sum  \\\n",
       "11511 -0.635676  0.384668       1.187188  0.293945  0.212348 -0.432277   \n",
       "7311  -0.635676 -1.062087      -0.869251  0.293945 -0.555743 -0.649618   \n",
       "4788   1.820554  0.822593       0.349859  0.293945 -0.488974  0.543728   \n",
       "16133 -0.635676 -0.255616       1.768796  0.293945 -1.643833  0.461458   \n",
       "13498 -0.635676  0.027636      -0.549389  0.293945 -1.324460 -3.465528   \n",
       "\n",
       "         w_odds  w_odds_allo       win  top3  \n",
       "11511 -1.295509     1.514223  1.043573     0  \n",
       "7311   1.645869    -1.368502  1.798651     0  \n",
       "4788  -1.268722     0.930715 -1.278335     0  \n",
       "16133 -0.375581     0.548316  0.812336     1  \n",
       "13498  0.681539    -0.715906 -0.370200     0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eda(display_format = 'bokeh')\n",
    "pd.set_option('display.max_columns', None)\n",
    "selected_data = get_config('dataset_transformed')\n",
    "selected_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be3a620b-3901-4c72-b8e9-b5d7e2fd2ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from autoviz.AutoViz_Class import AutoViz_Class\n",
    "\n",
    "# AV = AutoViz_Class()\n",
    "\n",
    "# report = AV.AutoViz(\n",
    "#     filename = '',\n",
    "#     dfte=selected_data,\n",
    "#     depVar='top3',\n",
    "#     verbose=0,\n",
    "#     lowess=False,\n",
    "#     chart_format='svg',\n",
    "#     max_rows_analyzed=1500,\n",
    "#     max_cols_analyzed=30\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a59f7b-ad37-42ca-902d-1f0c542c233b",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451662db-4624-4151-baa6-37b48fb7d068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_3_metrics(y, y_pred):\n",
    "    tp = np.where((y_pred==1) & (y==1), 60, 0)\n",
    "    fp = np.where((y_pred==1) & (y==0), -30, 0)\n",
    "    tn = np.where((y_pred==0) & (y==0), 0, 0)\n",
    "    fn = np.where((y_pred==0) & (y==1), -10, 0)\n",
    "    return np.sum([tp, fp, tn, fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1fb12ea-6377-4870-a3c9-5cb9298252a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                                                         T3_metric\n",
       "Display Name                                                 T3_metric\n",
       "Score Function       <pycaret.internal.metrics.EncodedDecodedLabels...\n",
       "Scorer               make_scorer(top_3_metrics, response_method='pr...\n",
       "Target                                                            pred\n",
       "Args                                                                {}\n",
       "Greater is Better                                                 True\n",
       "Multiclass                                                        True\n",
       "Custom                                                            True\n",
       "Name: T3_metric, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(reset = True)\n",
    "add_metric('T3_metric', 'T3_metric', top_3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2878927-9ffa-41db-803f-e41165c58d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>14:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      \n",
       "                                                                      \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                14:53:00\n",
       "Status     . . . . . . . . . . . . . . . . . .        Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Extra Trees Classifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_11254 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_11254_row0_col0, #T_11254_row0_col1, #T_11254_row0_col2, #T_11254_row0_col3, #T_11254_row0_col4, #T_11254_row0_col5, #T_11254_row0_col6, #T_11254_row0_col7, #T_11254_row0_col8, #T_11254_row0_col9, #T_11254_row1_col0, #T_11254_row1_col1, #T_11254_row1_col2, #T_11254_row1_col3, #T_11254_row1_col4, #T_11254_row1_col5, #T_11254_row1_col6, #T_11254_row1_col7, #T_11254_row1_col8, #T_11254_row1_col9, #T_11254_row2_col0, #T_11254_row2_col1, #T_11254_row2_col2, #T_11254_row2_col3, #T_11254_row2_col4, #T_11254_row2_col5, #T_11254_row2_col6, #T_11254_row2_col7, #T_11254_row2_col8, #T_11254_row2_col9, #T_11254_row3_col0, #T_11254_row3_col1, #T_11254_row3_col2, #T_11254_row3_col3, #T_11254_row3_col4, #T_11254_row3_col5, #T_11254_row3_col6, #T_11254_row3_col7, #T_11254_row3_col8, #T_11254_row3_col9, #T_11254_row4_col0, #T_11254_row4_col1, #T_11254_row4_col2, #T_11254_row4_col3, #T_11254_row4_col4, #T_11254_row4_col5, #T_11254_row4_col6, #T_11254_row4_col7, #T_11254_row4_col8, #T_11254_row4_col9, #T_11254_row5_col0, #T_11254_row5_col1, #T_11254_row5_col2, #T_11254_row5_col3, #T_11254_row5_col4, #T_11254_row5_col5, #T_11254_row5_col6, #T_11254_row5_col7, #T_11254_row5_col8, #T_11254_row5_col9, #T_11254_row6_col0, #T_11254_row6_col1, #T_11254_row6_col2, #T_11254_row6_col3, #T_11254_row6_col4, #T_11254_row6_col5, #T_11254_row6_col6, #T_11254_row6_col7, #T_11254_row6_col8, #T_11254_row6_col9, #T_11254_row7_col0, #T_11254_row7_col1, #T_11254_row7_col2, #T_11254_row7_col3, #T_11254_row7_col4, #T_11254_row7_col5, #T_11254_row7_col6, #T_11254_row7_col7, #T_11254_row7_col8, #T_11254_row7_col9, #T_11254_row8_col0, #T_11254_row8_col1, #T_11254_row8_col2, #T_11254_row8_col3, #T_11254_row8_col4, #T_11254_row8_col5, #T_11254_row8_col6, #T_11254_row8_col7, #T_11254_row8_col8, #T_11254_row8_col9, #T_11254_row9_col0, #T_11254_row9_col1, #T_11254_row9_col2, #T_11254_row9_col3, #T_11254_row9_col4, #T_11254_row9_col5, #T_11254_row9_col6, #T_11254_row9_col7, #T_11254_row9_col8, #T_11254_row9_col9, #T_11254_row10_col0, #T_11254_row10_col1, #T_11254_row10_col2, #T_11254_row10_col3, #T_11254_row10_col4, #T_11254_row10_col5, #T_11254_row10_col6, #T_11254_row10_col7, #T_11254_row10_col8, #T_11254_row10_col9 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_11254\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_11254_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_11254_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_11254_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_11254_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_11254_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_11254_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_11254_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_11254_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_11254_level0_col8\" class=\"col_heading level0 col8\" >T3_metric</th>\n",
       "      <th id=\"T_11254_level0_col9\" class=\"col_heading level0 col9\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row0\" class=\"row_heading level0 row0\" >lr</th>\n",
       "      <td id=\"T_11254_row0_col0\" class=\"data row0 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_11254_row0_col1\" class=\"data row0 col1\" >0.7462</td>\n",
       "      <td id=\"T_11254_row0_col2\" class=\"data row0 col2\" >0.6595</td>\n",
       "      <td id=\"T_11254_row0_col3\" class=\"data row0 col3\" >0.1994</td>\n",
       "      <td id=\"T_11254_row0_col4\" class=\"data row0 col4\" >0.3534</td>\n",
       "      <td id=\"T_11254_row0_col5\" class=\"data row0 col5\" >0.2522</td>\n",
       "      <td id=\"T_11254_row0_col6\" class=\"data row0 col6\" >0.1150</td>\n",
       "      <td id=\"T_11254_row0_col7\" class=\"data row0 col7\" >0.1224</td>\n",
       "      <td id=\"T_11254_row0_col8\" class=\"data row0 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row0_col9\" class=\"data row0 col9\" >82.9930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row1\" class=\"row_heading level0 row1\" >knn</th>\n",
       "      <td id=\"T_11254_row1_col0\" class=\"data row1 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_11254_row1_col1\" class=\"data row1 col1\" >0.6076</td>\n",
       "      <td id=\"T_11254_row1_col2\" class=\"data row1 col2\" >0.5402</td>\n",
       "      <td id=\"T_11254_row1_col3\" class=\"data row1 col3\" >0.3952</td>\n",
       "      <td id=\"T_11254_row1_col4\" class=\"data row1 col4\" >0.2481</td>\n",
       "      <td id=\"T_11254_row1_col5\" class=\"data row1 col5\" >0.3047</td>\n",
       "      <td id=\"T_11254_row1_col6\" class=\"data row1 col6\" >0.0510</td>\n",
       "      <td id=\"T_11254_row1_col7\" class=\"data row1 col7\" >0.0537</td>\n",
       "      <td id=\"T_11254_row1_col8\" class=\"data row1 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row1_col9\" class=\"data row1 col9\" >39.2950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row2\" class=\"row_heading level0 row2\" >nb</th>\n",
       "      <td id=\"T_11254_row2_col0\" class=\"data row2 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_11254_row2_col1\" class=\"data row2 col1\" >0.2032</td>\n",
       "      <td id=\"T_11254_row2_col2\" class=\"data row2 col2\" >0.4742</td>\n",
       "      <td id=\"T_11254_row2_col3\" class=\"data row2 col3\" >0.8793</td>\n",
       "      <td id=\"T_11254_row2_col4\" class=\"data row2 col4\" >0.1950</td>\n",
       "      <td id=\"T_11254_row2_col5\" class=\"data row2 col5\" >0.3191</td>\n",
       "      <td id=\"T_11254_row2_col6\" class=\"data row2 col6\" >-0.0025</td>\n",
       "      <td id=\"T_11254_row2_col7\" class=\"data row2 col7\" >-0.0176</td>\n",
       "      <td id=\"T_11254_row2_col8\" class=\"data row2 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row2_col9\" class=\"data row2 col9\" >42.3350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row3\" class=\"row_heading level0 row3\" >dt</th>\n",
       "      <td id=\"T_11254_row3_col0\" class=\"data row3 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_11254_row3_col1\" class=\"data row3 col1\" >0.6070</td>\n",
       "      <td id=\"T_11254_row3_col2\" class=\"data row3 col2\" >0.5045</td>\n",
       "      <td id=\"T_11254_row3_col3\" class=\"data row3 col3\" >0.3231</td>\n",
       "      <td id=\"T_11254_row3_col4\" class=\"data row3 col4\" >0.2664</td>\n",
       "      <td id=\"T_11254_row3_col5\" class=\"data row3 col5\" >0.2919</td>\n",
       "      <td id=\"T_11254_row3_col6\" class=\"data row3 col6\" >0.1013</td>\n",
       "      <td id=\"T_11254_row3_col7\" class=\"data row3 col7\" >0.1022</td>\n",
       "      <td id=\"T_11254_row3_col8\" class=\"data row3 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row3_col9\" class=\"data row3 col9\" >80.3270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row4\" class=\"row_heading level0 row4\" >svm</th>\n",
       "      <td id=\"T_11254_row4_col0\" class=\"data row4 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_11254_row4_col1\" class=\"data row4 col1\" >0.7492</td>\n",
       "      <td id=\"T_11254_row4_col2\" class=\"data row4 col2\" >0.5865</td>\n",
       "      <td id=\"T_11254_row4_col3\" class=\"data row4 col3\" >0.0940</td>\n",
       "      <td id=\"T_11254_row4_col4\" class=\"data row4 col4\" >0.2748</td>\n",
       "      <td id=\"T_11254_row4_col5\" class=\"data row4 col5\" >0.1382</td>\n",
       "      <td id=\"T_11254_row4_col6\" class=\"data row4 col6\" >0.0330</td>\n",
       "      <td id=\"T_11254_row4_col7\" class=\"data row4 col7\" >0.0394</td>\n",
       "      <td id=\"T_11254_row4_col8\" class=\"data row4 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row4_col9\" class=\"data row4 col9\" >80.6990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row5\" class=\"row_heading level0 row5\" >ridge</th>\n",
       "      <td id=\"T_11254_row5_col0\" class=\"data row5 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_11254_row5_col1\" class=\"data row5 col1\" >0.7776</td>\n",
       "      <td id=\"T_11254_row5_col2\" class=\"data row5 col2\" >0.6619</td>\n",
       "      <td id=\"T_11254_row5_col3\" class=\"data row5 col3\" >0.0272</td>\n",
       "      <td id=\"T_11254_row5_col4\" class=\"data row5 col4\" >0.3522</td>\n",
       "      <td id=\"T_11254_row5_col5\" class=\"data row5 col5\" >0.0500</td>\n",
       "      <td id=\"T_11254_row5_col6\" class=\"data row5 col6\" >0.0203</td>\n",
       "      <td id=\"T_11254_row5_col7\" class=\"data row5 col7\" >0.0424</td>\n",
       "      <td id=\"T_11254_row5_col8\" class=\"data row5 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row5_col9\" class=\"data row5 col9\" >81.6690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row6\" class=\"row_heading level0 row6\" >rf</th>\n",
       "      <td id=\"T_11254_row6_col0\" class=\"data row6 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_11254_row6_col1\" class=\"data row6 col1\" >0.7539</td>\n",
       "      <td id=\"T_11254_row6_col2\" class=\"data row6 col2\" >0.7225</td>\n",
       "      <td id=\"T_11254_row6_col3\" class=\"data row6 col3\" >0.2890</td>\n",
       "      <td id=\"T_11254_row6_col4\" class=\"data row6 col4\" >0.4082</td>\n",
       "      <td id=\"T_11254_row6_col5\" class=\"data row6 col5\" >0.3381</td>\n",
       "      <td id=\"T_11254_row6_col6\" class=\"data row6 col6\" >0.1925</td>\n",
       "      <td id=\"T_11254_row6_col7\" class=\"data row6 col7\" >0.1969</td>\n",
       "      <td id=\"T_11254_row6_col8\" class=\"data row6 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row6_col9\" class=\"data row6 col9\" >81.8630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row7\" class=\"row_heading level0 row7\" >qda</th>\n",
       "      <td id=\"T_11254_row7_col0\" class=\"data row7 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_11254_row7_col1\" class=\"data row7 col1\" >0.4229</td>\n",
       "      <td id=\"T_11254_row7_col2\" class=\"data row7 col2\" >0.5084</td>\n",
       "      <td id=\"T_11254_row7_col3\" class=\"data row7 col3\" >0.6449</td>\n",
       "      <td id=\"T_11254_row7_col4\" class=\"data row7 col4\" >0.2201</td>\n",
       "      <td id=\"T_11254_row7_col5\" class=\"data row7 col5\" >0.3219</td>\n",
       "      <td id=\"T_11254_row7_col6\" class=\"data row7 col6\" >0.0043</td>\n",
       "      <td id=\"T_11254_row7_col7\" class=\"data row7 col7\" >0.0050</td>\n",
       "      <td id=\"T_11254_row7_col8\" class=\"data row7 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row7_col9\" class=\"data row7 col9\" >81.4250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row8\" class=\"row_heading level0 row8\" >ada</th>\n",
       "      <td id=\"T_11254_row8_col0\" class=\"data row8 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_11254_row8_col1\" class=\"data row8 col1\" >0.2958</td>\n",
       "      <td id=\"T_11254_row8_col2\" class=\"data row8 col2\" >0.7377</td>\n",
       "      <td id=\"T_11254_row8_col3\" class=\"data row8 col3\" >0.9988</td>\n",
       "      <td id=\"T_11254_row8_col4\" class=\"data row8 col4\" >0.2360</td>\n",
       "      <td id=\"T_11254_row8_col5\" class=\"data row8 col5\" >0.3818</td>\n",
       "      <td id=\"T_11254_row8_col6\" class=\"data row8 col6\" >0.0457</td>\n",
       "      <td id=\"T_11254_row8_col7\" class=\"data row8 col7\" >0.1516</td>\n",
       "      <td id=\"T_11254_row8_col8\" class=\"data row8 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row8_col9\" class=\"data row8 col9\" >81.7010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row9\" class=\"row_heading level0 row9\" >gbc</th>\n",
       "      <td id=\"T_11254_row9_col0\" class=\"data row9 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_11254_row9_col1\" class=\"data row9 col1\" >0.7627</td>\n",
       "      <td id=\"T_11254_row9_col2\" class=\"data row9 col2\" >0.7495</td>\n",
       "      <td id=\"T_11254_row9_col3\" class=\"data row9 col3\" >0.3315</td>\n",
       "      <td id=\"T_11254_row9_col4\" class=\"data row9 col4\" >0.4393</td>\n",
       "      <td id=\"T_11254_row9_col5\" class=\"data row9 col5\" >0.3773</td>\n",
       "      <td id=\"T_11254_row9_col6\" class=\"data row9 col6\" >0.2346</td>\n",
       "      <td id=\"T_11254_row9_col7\" class=\"data row9 col7\" >0.2383</td>\n",
       "      <td id=\"T_11254_row9_col8\" class=\"data row9 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row9_col9\" class=\"data row9 col9\" >81.5250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11254_level0_row10\" class=\"row_heading level0 row10\" >lda</th>\n",
       "      <td id=\"T_11254_row10_col0\" class=\"data row10 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_11254_row10_col1\" class=\"data row10 col1\" >0.7491</td>\n",
       "      <td id=\"T_11254_row10_col2\" class=\"data row10 col2\" >0.6622</td>\n",
       "      <td id=\"T_11254_row10_col3\" class=\"data row10 col3\" >0.2023</td>\n",
       "      <td id=\"T_11254_row10_col4\" class=\"data row10 col4\" >0.3647</td>\n",
       "      <td id=\"T_11254_row10_col5\" class=\"data row10 col5\" >0.2578</td>\n",
       "      <td id=\"T_11254_row10_col6\" class=\"data row10 col6\" >0.1226</td>\n",
       "      <td id=\"T_11254_row10_col7\" class=\"data row10 col7\" >0.1310</td>\n",
       "      <td id=\"T_11254_row10_col8\" class=\"data row10 col8\" >0.0000</td>\n",
       "      <td id=\"T_11254_row10_col9\" class=\"data row10 col9\" >114.9050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d4b4b3c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edf126ecd6b43338db7f2dad9a24671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\pycaret\\utils\\generic.py:964\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m globals_d[name] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\pycaret\\classification\\functional.py:814\u001b[0m, in \u001b[0;36mcompare_models\u001b[1;34m(include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, engine, verbose, parallel)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;129m@check_if_global_is_not_none\u001b[39m(\u001b[38;5;28mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompare_models\u001b[39m(\n\u001b[0;32m    674\u001b[0m     include: Optional[List[Union[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m     parallel: Optional[ParallelBackend] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    691\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Any, List[Any]]:\n\u001b[0;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    This function trains and evaluates performance of all estimators available in the\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m    model library using cross validation. The output of this function is a score grid\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m    - No models are logged in ``MLFlow`` when ``cross_validation`` parameter is False.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_CURRENT_EXPERIMENT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbudget_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mturbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobability_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\pycaret\\classification\\oop.py:1180\u001b[0m, in \u001b[0;36mClassificationExperiment.compare_models\u001b[1;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, engine, verbose, parallel)\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_engine(estimator\u001b[38;5;241m=\u001b[39mestimator, engine\u001b[38;5;241m=\u001b[39meng, severity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1180\u001b[0m     return_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbudget_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mturbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobability_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaller_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1201\u001b[0m         \u001b[38;5;66;03m# Reset the models back to the default engines\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:794\u001b[0m, in \u001b[0;36m_SupervisedExperiment.compare_models\u001b[1;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, verbose, parallel, caller_params)\u001b[0m\n\u001b[0;32m    791\u001b[0m results_columns_to_ignore \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoff\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 794\u001b[0m     model, model_fit_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcreate_model_args)\n\u001b[0;32m    795\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpull(pop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    797\u001b[0m         np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m    798\u001b[0m             model_results\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    803\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1533\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model\u001b[1;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model, model_fit_time\n\u001b[0;32m   1531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m-> 1533\u001b[0m model, model_fit_time, model_results, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# end runtime\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m runtime_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1126\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model_with_cv\u001b[1;34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score)\u001b[0m\n\u001b[0;32m   1124\u001b[0m     model_fit_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m redirect_output(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger):\n\u001b[1;32m-> 1126\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpipeline_with_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m model_fit_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1140\u001b[0m model_fit_time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(model_fit_end \u001b[38;5;241m-\u001b[39m model_fit_start)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 430\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sing\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\horseracing-prediction-oibbYk1R-py3.10\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compare baseline models\n",
    "top3 = compare_models(\n",
    "                    n_select = 3,\n",
    "                    sort = 'T3_metric',\n",
    "                    probability_threshold = 0.4,\n",
    "                    exclude = 'et'\n",
    "                    )\n",
    "\n",
    "tuned_top3 = [tune_model(i) for i in top3]\n",
    "blender = blend_models(tuned_top3, choose_better = True, probability_threshold = 0.2)\n",
    "stacker = stack_models(tuned_top3, choose_better = True, probability_threshold = 0.2)\n",
    "best_top3_model = automl(optimize = 'Prec.', turbo = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c4a1db-5f53-443d-985e-660f87ec6fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________   confusion_matrix   ___________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m available_plots:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_________________________________________  \u001b[39m\u001b[38;5;124m'\u001b[39m,i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  ___________________________________________\u001b[39m\u001b[38;5;124m'\u001b[39m,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mplot_model\u001b[49m(best_top3_model, plot\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_model' is not defined"
     ]
    }
   ],
   "source": [
    "available_plots = ['confusion_matrix', 'feature']\n",
    "    \n",
    "for i in available_plots:\n",
    "    print('_________________________________________  ',i,'  ___________________________________________',sep=' ')\n",
    "    plot_model(best_top3_model, plot=i)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8ff1a6e-1c89-4c4a-97df-a774a926c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=FastMemory(location=/tmp/joblib),\n",
       "          steps=[('iterative_imputer',\n",
       "                  TransformerWrapper(exclude=None, include=None,\n",
       "                                     transformer=IterativeImputer(add_indicator=False,\n",
       "                                                                  cat_estimator=LGBMClassifier(boosting_type='gbdt',\n",
       "                                                                                               class_weight=None,\n",
       "                                                                                               colsample_bytree=1.0,\n",
       "                                                                                               importance_type='split',\n",
       "                                                                                               learning_rate=0.1,\n",
       "                                                                                               max_depth=-1,\n",
       "                                                                                               min_child_samples=20,\n",
       "                                                                                               min_child_weig...\n",
       "                 ('clean_column_names',\n",
       "                  TransformerWrapper(exclude=None, include=None,\n",
       "                                     transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n",
       "                 ('trained_model',\n",
       "                  CustomProbabilityThresholdClassifier(border_count=254,\n",
       "                                                       classifier=<catboost.core.CatBoostClassifier object at 0x7f10ed02e430>,\n",
       "                                                       probability_threshold=0.3,\n",
       "                                                       random_state=42,\n",
       "                                                       task_type='CPU',\n",
       "                                                       verbose=False))],\n",
       "          verbose=False),\n",
       " 'Test_top3_model_112023.pkl')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(top3[1], 'Test_top3_model_112023')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16798b82-baab-49bd-995d-6a251b328602",
   "metadata": {},
   "source": [
    "## Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdec753-3f69-4235-a51c-2313a44a38dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ad63b_row10_col0, #T_ad63b_row10_col1, #T_ad63b_row10_col2, #T_ad63b_row10_col3, #T_ad63b_row10_col4, #T_ad63b_row10_col5, #T_ad63b_row10_col6, #T_ad63b_row10_col7 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ad63b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ad63b_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_ad63b_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_ad63b_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_ad63b_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_ad63b_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_ad63b_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_ad63b_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "      <th id=\"T_ad63b_level0_col7\" class=\"col_heading level0 col7\" >T3_metric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ad63b_row0_col0\" class=\"data row0 col0\" >0.7488</td>\n",
       "      <td id=\"T_ad63b_row0_col1\" class=\"data row0 col1\" >0.7487</td>\n",
       "      <td id=\"T_ad63b_row0_col2\" class=\"data row0 col2\" >0.4305</td>\n",
       "      <td id=\"T_ad63b_row0_col3\" class=\"data row0 col3\" >0.4286</td>\n",
       "      <td id=\"T_ad63b_row0_col4\" class=\"data row0 col4\" >0.4295</td>\n",
       "      <td id=\"T_ad63b_row0_col5\" class=\"data row0 col5\" >0.2684</td>\n",
       "      <td id=\"T_ad63b_row0_col6\" class=\"data row0 col6\" >0.2684</td>\n",
       "      <td id=\"T_ad63b_row0_col7\" class=\"data row0 col7\" >650.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ad63b_row1_col0\" class=\"data row1 col0\" >0.7704</td>\n",
       "      <td id=\"T_ad63b_row1_col1\" class=\"data row1 col1\" >0.7700</td>\n",
       "      <td id=\"T_ad63b_row1_col2\" class=\"data row1 col2\" >0.4574</td>\n",
       "      <td id=\"T_ad63b_row1_col3\" class=\"data row1 col3\" >0.4766</td>\n",
       "      <td id=\"T_ad63b_row1_col4\" class=\"data row1 col4\" >0.4668</td>\n",
       "      <td id=\"T_ad63b_row1_col5\" class=\"data row1 col5\" >0.3206</td>\n",
       "      <td id=\"T_ad63b_row1_col6\" class=\"data row1 col6\" >0.3207</td>\n",
       "      <td id=\"T_ad63b_row1_col7\" class=\"data row1 col7\" >1550.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ad63b_row2_col0\" class=\"data row2 col0\" >0.7448</td>\n",
       "      <td id=\"T_ad63b_row2_col1\" class=\"data row2 col1\" >0.7446</td>\n",
       "      <td id=\"T_ad63b_row2_col2\" class=\"data row2 col2\" >0.3857</td>\n",
       "      <td id=\"T_ad63b_row2_col3\" class=\"data row2 col3\" >0.4135</td>\n",
       "      <td id=\"T_ad63b_row2_col4\" class=\"data row2 col4\" >0.3991</td>\n",
       "      <td id=\"T_ad63b_row2_col5\" class=\"data row2 col5\" >0.2373</td>\n",
       "      <td id=\"T_ad63b_row2_col6\" class=\"data row2 col6\" >0.2376</td>\n",
       "      <td id=\"T_ad63b_row2_col7\" class=\"data row2 col7\" >130.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ad63b_row3_col0\" class=\"data row3 col0\" >0.7823</td>\n",
       "      <td id=\"T_ad63b_row3_col1\" class=\"data row3 col1\" >0.7522</td>\n",
       "      <td id=\"T_ad63b_row3_col2\" class=\"data row3 col2\" >0.4170</td>\n",
       "      <td id=\"T_ad63b_row3_col3\" class=\"data row3 col3\" >0.5054</td>\n",
       "      <td id=\"T_ad63b_row3_col4\" class=\"data row3 col4\" >0.4570</td>\n",
       "      <td id=\"T_ad63b_row3_col5\" class=\"data row3 col5\" >0.3224</td>\n",
       "      <td id=\"T_ad63b_row3_col6\" class=\"data row3 col6\" >0.3247</td>\n",
       "      <td id=\"T_ad63b_row3_col7\" class=\"data row3 col7\" >1550.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ad63b_row4_col0\" class=\"data row4 col0\" >0.7724</td>\n",
       "      <td id=\"T_ad63b_row4_col1\" class=\"data row4 col1\" >0.7751</td>\n",
       "      <td id=\"T_ad63b_row4_col2\" class=\"data row4 col2\" >0.4395</td>\n",
       "      <td id=\"T_ad63b_row4_col3\" class=\"data row4 col3\" >0.4804</td>\n",
       "      <td id=\"T_ad63b_row4_col4\" class=\"data row4 col4\" >0.4590</td>\n",
       "      <td id=\"T_ad63b_row4_col5\" class=\"data row4 col5\" >0.3153</td>\n",
       "      <td id=\"T_ad63b_row4_col6\" class=\"data row4 col6\" >0.3158</td>\n",
       "      <td id=\"T_ad63b_row4_col7\" class=\"data row4 col7\" >1450.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_ad63b_row5_col0\" class=\"data row5 col0\" >0.7310</td>\n",
       "      <td id=\"T_ad63b_row5_col1\" class=\"data row5 col1\" >0.7295</td>\n",
       "      <td id=\"T_ad63b_row5_col2\" class=\"data row5 col2\" >0.4215</td>\n",
       "      <td id=\"T_ad63b_row5_col3\" class=\"data row5 col3\" >0.3950</td>\n",
       "      <td id=\"T_ad63b_row5_col4\" class=\"data row5 col4\" >0.4078</td>\n",
       "      <td id=\"T_ad63b_row5_col5\" class=\"data row5 col5\" >0.2341</td>\n",
       "      <td id=\"T_ad63b_row5_col6\" class=\"data row5 col6\" >0.2343</td>\n",
       "      <td id=\"T_ad63b_row5_col7\" class=\"data row5 col7\" >30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_ad63b_row6_col0\" class=\"data row6 col0\" >0.7860</td>\n",
       "      <td id=\"T_ad63b_row6_col1\" class=\"data row6 col1\" >0.7755</td>\n",
       "      <td id=\"T_ad63b_row6_col2\" class=\"data row6 col2\" >0.4234</td>\n",
       "      <td id=\"T_ad63b_row6_col3\" class=\"data row6 col3\" >0.5137</td>\n",
       "      <td id=\"T_ad63b_row6_col4\" class=\"data row6 col4\" >0.4642</td>\n",
       "      <td id=\"T_ad63b_row6_col5\" class=\"data row6 col5\" >0.3320</td>\n",
       "      <td id=\"T_ad63b_row6_col6\" class=\"data row6 col6\" >0.3345</td>\n",
       "      <td id=\"T_ad63b_row6_col7\" class=\"data row6 col7\" >1690.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_ad63b_row7_col0\" class=\"data row7 col0\" >0.7653</td>\n",
       "      <td id=\"T_ad63b_row7_col1\" class=\"data row7 col1\" >0.7570</td>\n",
       "      <td id=\"T_ad63b_row7_col2\" class=\"data row7 col2\" >0.4081</td>\n",
       "      <td id=\"T_ad63b_row7_col3\" class=\"data row7 col3\" >0.4619</td>\n",
       "      <td id=\"T_ad63b_row7_col4\" class=\"data row7 col4\" >0.4333</td>\n",
       "      <td id=\"T_ad63b_row7_col5\" class=\"data row7 col5\" >0.2860</td>\n",
       "      <td id=\"T_ad63b_row7_col6\" class=\"data row7 col6\" >0.2869</td>\n",
       "      <td id=\"T_ad63b_row7_col7\" class=\"data row7 col7\" >960.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_ad63b_row8_col0\" class=\"data row8 col0\" >0.7751</td>\n",
       "      <td id=\"T_ad63b_row8_col1\" class=\"data row8 col1\" >0.7917</td>\n",
       "      <td id=\"T_ad63b_row8_col2\" class=\"data row8 col2\" >0.4843</td>\n",
       "      <td id=\"T_ad63b_row8_col3\" class=\"data row8 col3\" >0.4887</td>\n",
       "      <td id=\"T_ad63b_row8_col4\" class=\"data row8 col4\" >0.4865</td>\n",
       "      <td id=\"T_ad63b_row8_col5\" class=\"data row8 col5\" >0.3426</td>\n",
       "      <td id=\"T_ad63b_row8_col6\" class=\"data row8 col6\" >0.3426</td>\n",
       "      <td id=\"T_ad63b_row8_col7\" class=\"data row8 col7\" >1940.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_ad63b_row9_col0\" class=\"data row9 col0\" >0.7751</td>\n",
       "      <td id=\"T_ad63b_row9_col1\" class=\"data row9 col1\" >0.7938</td>\n",
       "      <td id=\"T_ad63b_row9_col2\" class=\"data row9 col2\" >0.4664</td>\n",
       "      <td id=\"T_ad63b_row9_col3\" class=\"data row9 col3\" >0.4883</td>\n",
       "      <td id=\"T_ad63b_row9_col4\" class=\"data row9 col4\" >0.4771</td>\n",
       "      <td id=\"T_ad63b_row9_col5\" class=\"data row9 col5\" >0.3339</td>\n",
       "      <td id=\"T_ad63b_row9_col6\" class=\"data row9 col6\" >0.3341</td>\n",
       "      <td id=\"T_ad63b_row9_col7\" class=\"data row9 col7\" >1780.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_ad63b_row10_col0\" class=\"data row10 col0\" >0.7651</td>\n",
       "      <td id=\"T_ad63b_row10_col1\" class=\"data row10 col1\" >0.7638</td>\n",
       "      <td id=\"T_ad63b_row10_col2\" class=\"data row10 col2\" >0.4334</td>\n",
       "      <td id=\"T_ad63b_row10_col3\" class=\"data row10 col3\" >0.4652</td>\n",
       "      <td id=\"T_ad63b_row10_col4\" class=\"data row10 col4\" >0.4480</td>\n",
       "      <td id=\"T_ad63b_row10_col5\" class=\"data row10 col5\" >0.2993</td>\n",
       "      <td id=\"T_ad63b_row10_col6\" class=\"data row10 col6\" >0.3000</td>\n",
       "      <td id=\"T_ad63b_row10_col7\" class=\"data row10 col7\" >1173.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ad63b_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_ad63b_row11_col0\" class=\"data row11 col0\" >0.0169</td>\n",
       "      <td id=\"T_ad63b_row11_col1\" class=\"data row11 col1\" >0.0199</td>\n",
       "      <td id=\"T_ad63b_row11_col2\" class=\"data row11 col2\" >0.0278</td>\n",
       "      <td id=\"T_ad63b_row11_col3\" class=\"data row11 col3\" >0.0379</td>\n",
       "      <td id=\"T_ad63b_row11_col4\" class=\"data row11 col4\" >0.0278</td>\n",
       "      <td id=\"T_ad63b_row11_col5\" class=\"data row11 col5\" >0.0382</td>\n",
       "      <td id=\"T_ad63b_row11_col6\" class=\"data row11 col6\" >0.0385</td>\n",
       "      <td id=\"T_ad63b_row11_col7\" class=\"data row11 col7\" >655.6074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0000485160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7232dff2c7674c6090cdd54799cd3d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>02:12:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Searching Hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            \n",
       "                                                                            \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                      02:12:18\n",
       "Status     . . . . . . . . . . . . . . . . . .     Searching Hyperparameters\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Gradient Boosting Classifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9e53ac624d4ee2984898dfcef3d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-11-22 02:12:18,393]\u001b[0m Searching the best hyperparameters using 10146 samples...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "catboost_model = create_model('gbc',\n",
    "                        probability_threshold = 0.35\n",
    "                       )\n",
    "tuned_rf = tune_model(catboost_model, \n",
    "                     optimize = 'AUC',\n",
    "                     search_library = 'optuna',\n",
    "                     n_iter = 20,\n",
    "                     # search_algorithm = 'optuna'\n",
    "                     early_stopping = True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754464fa-c43a-49dd-b793-7b9a98c8da02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_plots = ['confusion_matrix', 'feature','auc']\n",
    "    \n",
    "for i in available_plots:\n",
    "    print('_________________________________________  ',i,'  ___________________________________________',sep=' ')\n",
    "    plot_model(tuned_rf, plot=i)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7534ee4e-0522-4291-834e-7d9502b7904c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=FastMemory(location=/tmp/joblib),\n",
       "          steps=[('iterative_imputer',\n",
       "                  TransformerWrapper(exclude=None, include=None,\n",
       "                                     transformer=IterativeImputer(add_indicator=False,\n",
       "                                                                  cat_estimator=LGBMClassifier(boosting_type='gbdt',\n",
       "                                                                                               class_weight=None,\n",
       "                                                                                               colsample_bytree=1.0,\n",
       "                                                                                               importance_type='split',\n",
       "                                                                                               learning_rate=0.1,\n",
       "                                                                                               max_depth=-1,\n",
       "                                                                                               min_child_samples=20,\n",
       "                                                                                               min_child_weig...\n",
       "                                                       learning_rate=0.1,\n",
       "                                                       loss='log_loss',\n",
       "                                                       max_depth=3,\n",
       "                                                       max_features=None,\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_samples_leaf=1,\n",
       "                                                       min_samples_split=2,\n",
       "                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                       n_estimators=100,\n",
       "                                                       n_iter_no_change=None,\n",
       "                                                       probability_threshold=0.35,\n",
       "                                                       random_state=42,\n",
       "                                                       subsample=1.0, tol=0.0001,\n",
       "                                                       validation_fraction=0.1,\n",
       "                                                       verbose=0,\n",
       "                                                       warm_start=False))],\n",
       "          verbose=False),\n",
       " 'gbc_top3_model_112023.pkl')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(tuned_rf, 'gbc_top3_model_112023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0559a-ba47-4b30-99e3-03e7a31f11cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "horseracing-prediction-oibbYk1R-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
